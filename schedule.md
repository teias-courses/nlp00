---
hide_hero: true
layout: page
hide_hero: true
show_sidebar: false
---

# Course Schedule

## Esfand 1400

| Session 	| Date	| Topic |
|------|------|------|
| 1 | 23 Bahman | Introduction	| 
| 2 | 25 Bahman | Semantic representation | 
| 3 | 30 Bahman | Word embeddings	(Word2vec) | 
| 4 | 2 Esfand  | Word embeddings (Evaluation, cross-lingual space, ambiguity and sense embeddings)	| 
| 5 | 7 Esfand  | Word embeddings (Sub-word embeddings, retrofitting, debiasing) <br> Readings: [[nn4nlp2021]](http://www.phontron.com/class/nn4nlp2021/schedule/wordemb.html)| 
| 6 | 9 Esfand  | Pytorch tutorial| 
| 7 | 14 Esfand | Language modeling	(n-gram, probability computation, back-off interpolation, sparsity and smoothing, feedforward NN for LM) |
| 8 | 16 Esfand | Language modeling with RNNs	(backprop through time, text generation, perplexity, text generation, sampling with temprature) |
| 9 | 21 Esfand | Vanishing/exploding gradients and fancy RNNs (LSTMs, bidirectional and stacked RNNs) |
| 10 | 23 Esfand | Machine Translation (SMT, NMT, seq2seq models, beam-search decoding, evaluation) |


## Farvardin 1401

| Session 	| Date	| Topic |
|------|------|------|
| 11 | 15 Farvardin | Attention mechanism (seq2seq attention, attention variants, hierarchical attention networks) |
| 12 | 20 Farvardin | Transformers (BERT model, self-attention, multi-head, positional encoding, contextualised embeddings, derivatives of BERT) |
| 13 | 22 Farvardin | Pytorch tutorial |
| 14 | 27 Farvardin | More about Transformers and Pretraining (subwords, byte-pair encoding, pretrain/finetune, architecture types: decoders, encoders, and encoder-decoders) |
| 15 | 29 Farvardin | Pytorch tutorial |

## Ordibehesht 1401

| Session 	| Date	| Topic |
|------|------|------|
| 16 | 5 Ordibehesht | \*Ethical Considerations and Bias in NLP (Zakizadeh & Eskanadari) |
| 17 | 10 Ordibehesht | Question Answering (reading comprehension, SQuAD, LSTM-based and BERT models, BiDAF, open-domain QA) |
| 18 | 17 Ordibehesht | \*Isotropicity of Semantic Spaces (Sara Rajaee) |
| 19 | 21 Ordibehesht | Modal analysis and explanation |
| 20 | 24 Ordibehesht | **Progress Report I** |
| 21 | 26 Ordibehesht | Few-shot, Zero-shot, and Prompt-based learning |
| 22 | 31 Ordibehesht | \*Zero-shot applictions of Cloze test (Tabasi)  |

## Khordad 1401

| Session 	| Date	| Topic |
|------|------|------|
| 23 | 2 Khordad | Integrating knowledge in language models (knowledge-aware LMs, entity embedding, ERNIE, memory-based models, KGLM, kNN-LM, modified training, WKLM, evaluation, prompting) |
| 24 | 7 Khordad | \*Interpretability (Modaressi & Mohebbi) |
| 25 | 9 Khordad | Neural Language Generation (applications, maximum likelihood training, teacher forcing, greedy and random sampling, top-k and nucleus sampling, unlikelihood training, exposure bias, evaluating NLG, bias and ethical concerns) |
| 26 | 18 Khordad | **Progress Report II** |



