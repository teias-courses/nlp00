---
hide_hero: true
layout: page
hide_hero: true
show_sidebar: false
---

# Course Schedule

## Esfand 1400

| Session 	| Date	| Topic | Notes |
|------|------|------|------|
| 1 | 23 Bahman | Introduction	| [[slides]](https://github.com/teias-courses/nlp00/blob/gh-pages/slides/C1-Introduction.pdf) |
| 2 | 25 Bahman | Semantic representation | [[slides]](https://github.com/teias-courses/nlp00/blob/gh-pages/slides/C2-Semantic_Representation.pdf) (Classes 2-5) |
| 3 | 30 Bahman | Word embeddings	(Word2vec) | |
| 4 | 2 Esfand  | Word embeddings (Evaluation, cross-lingual space, ambiguity and sense embeddings, sub-word embeddings, retrofitting, bias)	| |
| 5 | 7 Esfand  | Language modeling	(n-gram, probability computation, feedforward NN for LM) | Deep Learning Quiz |
| 6 | 9 Esfand  | *Pytorch tutorial*| HW#1 <br /> [[notebook]](https://github.com/teias-courses/nlp00/raw/gh-pages/resources/PyTorch_Tutorial_1.ipynb) |
| 7 | 14 Esfand | Language modeling with RNNs	(backprop through time, text generation, perplexity) |[[slides]](https://github.com/teias-courses/nlp00/blob/gh-pages/slides/C3-RNNS_and_LMs.pdf) (classes 7-9)|
| 8 | 16 Esfand | Vanishing gradients and fancy RNNs (LSTMs, bidirectional and stacked RNNs) ||
| 9 | 21 Esfand | Attention mechanism (seq2seq attention, attention variants) ||
| 10 | 23 Esfand | Transformers (BERT model, self-attention, multi-head, positional encoding, contextualised embeddings) ||


## Farvardin 1401

| Session 	| Date	| Topic | Notes |
|------|------|------|------|
| 11 | 15 Farvardin | More about Transformers and Pretraining (subwords, byte-pair encoding, pretrain/finetune) ||
| 12 | 20 Farvardin | *Midterm exam* ||
| 13 | 22 Farvardin | *Pytorch tutorial* ||
| 14 | 27 Farvardin | Transformers: derivatives of BERT and architecture types (decoders, encoders, and encoder-decoders) | HW#2 |
| 15 | 29 Farvardin | *Pytorch tutorial* | Project Proposal |

## Ordibehesht 1401

| Session 	| Date	| Topic | Notes |
|------|------|------|------|
| 16 | 5 Ordibehesht | \*Ethical Considerations and Bias in NLP (Zakizadeh & Eskanadari) ||
| 17 | 10 Ordibehesht | Question Answering (reading comprehension, SQuAD, LSTM-based and BERT models, BiDAF, open-domain QA) ||
| 18 | 17 Ordibehesht | \*Isotropicity of Semantic Spaces (Sara Rajaee) | HW#3 |
| 19 | 21 Ordibehesht | Model analysis and explanation ||
| 20 | 24 Ordibehesht | **Progress Report I** ||
| 21 | 26 Ordibehesht | Few-shot, Zero-shot, and Prompt-based learning ||
| 22 | 31 Ordibehesht | \*Zero-shot applictions of Cloze test (Tabasi)  ||

## Khordad 1401

| Session 	| Date	| Topic | Notes |
|------|------|------|------|
| 23 | 2 Khordad | Integrating knowledge in language models (knowledge-aware LMs, entity embedding, ERNIE, memory-based models, KGLM, kNN-LM, modified training, WKLM, evaluation, prompting) ||
| 24 | 7 Khordad | \*Interpretability (Modaressi & Mohebbi) ||
| 25 | 9 Khordad | Neural Language Generation (applications, maximum likelihood training, teacher forcing, greedy and random sampling, top-k and nucleus sampling, unlikelihood training, exposure bias, evaluating NLG, bias and ethical concerns) | HW#4 |
| 26 | 18 Khordad | **Progress Report II** ||
