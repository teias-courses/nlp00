---
hide_hero: true
layout: page
hide_hero: true
show_sidebar: false
---

# Course Schedule

## Esfand 1400

| Session 	| Date	| Topic | Notes |
|------|------|------|------|
| 1 | 23 Bahman | Introduction	| [[slides]](https://github.com/teias-courses/nlp00/blob/gh-pages/slides/C1-Introduction.pdf) |
| 2 | 25 Bahman | Semantic representation | [[slides]](https://github.com/teias-courses/nlp00/blob/gh-pages/slides/C2-Semantic_Representation.pdf) (Classes 2-5) |
| 3 | 30 Bahman | Word embeddings	(Word2vec) | |
| 4 | 2 Esfand  | Word embeddings (Evaluation, cross-lingual space, ambiguity and sense embeddings, sub-word embeddings, retrofitting, bias)	| |
| 5 | 7 Esfand  | Language modeling	(n-gram, probability computation, feedforward NN for LM) | Deep Learning Quiz |
| 6 | 9 Esfand  | *Pytorch tutorial*| HW#1 <br /> [[class notebook]](https://github.com/teias-courses/nlp00/raw/gh-pages/resources/Practical_NLP_Tutorial_Session_1.ipynb) |
| 7 | 14 Esfand | Language modeling with RNNs	(backprop through time, text generation, perplexity) |[[slides]](https://github.com/teias-courses/nlp00/blob/gh-pages/slides/C3-RNNS_and_LMs.pdf) (classes 7-9)|
| 8 | 16 Esfand | Vanishing gradients and fancy RNNs (LSTMs, bidirectional and stacked RNNs) ||
| 9 | 21 Esfand | Attention mechanism (seq2seq attention, attention variants) ||
| 10 | 23 Esfand | Transformers (self-attention, multi-head, positional encoding) | [[slides]](https://github.com/teias-courses/nlp00/blob/gh-pages/slides/C4-Transformers_and_BERT.pdf) (Classes 10-11)|


## Farvardin 1401

| Session 	| Date	| Topic | Notes |
|------|------|------|------|
| 11 | 15 Farvardin | More about Transformers (contextualised embeddings, MLM, and BERT, and pretrain/finetune)| |
| 12 | 20 Farvardin | Transformers: derivatives of BERT and architecture types (subwords and tokenization, decoders, encoders, and encoder-decoders) | [[slides]](https://github.com/teias-courses/nlp00/blob/gh-pages/slides/C5-More_About_Transformers.pdf)|
| 13 | 22 Farvardin | *Midterm exam* ||
| 14 | 27 Farvardin | *Pytorch tutorial* | HW#2 <br /> [[class notebook]](https://github.com/teias-courses/nlp00/raw/gh-pages/resources/Practical_NLP_Tutorial_Session_2.ipynb) |
| 15 | 29 Farvardin | *Pytorch tutorial* | Project Proposal <br /> [[class notebook]](https://github.com/teias-courses/nlp00/raw/gh-pages/resources/Practical_NLP_Tutorial_Session_3.ipynb) |

## Ordibehesht 1401

| Session 	| Date	| Topic | Notes |
|------|------|------|------|
| 16 | 5 Ordibehesht | Multilingual Learning (Data balancing, adapters and MAD-X, TLM, cross-lingual transfer learning, zero-shot, and active learning) | [[slides]](https://github.com/teias-courses/nlp00/blob/gh-pages/slides/C6-Multilingual_Learning.pdf)|
| 17 | 10 Ordibehesht |  Project Proposal ||
| 18 | 17 Ordibehesht | \*Isotropicity of Semantic Spaces (Sara Rajaee) | HW#3 |
| 19 | 21 Ordibehesht | Model analysis and explanation ||
| 20 | 24 Ordibehesht |  Integrating knowledge in language models (knowledge-aware LMs, entity embedding, ERNIE, memory-based models, KGLM, kNN-LM, modified training, WKLM, evaluation, prompting) ||
| 21 | 26 Ordibehesht | **Progress Report I** ||
| 22 | 31 Ordibehesht | \* Few-shot, Zero-shot, and Prompt-based learning (Mohsen Tabasi)  ||


## Khordad 1401

| Session 	| Date	| Topic | Notes |
|------|------|------|------|
| 23 | 2 Khordad | \*Ethical Considerations and Bias in NLP (Zakizadeh & Eskanadari) ||
| 24 | 7 Khordad | Question Answering (reading comprehension, SQuAD, LSTM-based and BERT models, BiDAF, open-domain QA) ||
| 25 | 9 Khordad | \*Interpretability (Modaressi & Mohebbi)| HW#4 |
| 26 | 16 Khordad | Neural Language Generation (applications, maximum likelihood training, teacher forcing, greedy and random sampling, top-k and nucleus sampling, unlikelihood training, exposure bias, evaluating NLG, bias and ethical concerns)  ||
| 27 | 21 Khordad | **Progress Report II** ||
