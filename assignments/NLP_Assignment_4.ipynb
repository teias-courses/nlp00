{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment_4.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "t2dukC-zjocR",
        "U81Yf1yvzc0d",
        "rvLeULP3bjbQ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKlAVvGvsG8i"
      },
      "source": [
        "# NLP Assignment #04 - Transformers to the rescue!\n",
        "Deep Learning / Spring 1400, Khatam University\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-_PRgRX1NDO"
      },
      "source": [
        "**Please pay attention to these notes:**\n",
        "<br><br>\n",
        "\n",
        "\n",
        "- **Assignment Due:** <b><font color='red'>1400.03.18</font></b> 23:59:00\n",
        "- If you need any additional information, please review the assignment page on the course website.\n",
        "- The items you need to answer are highlighted in <font color=\"SeaGreen\">**bold SeaGreen**</font> and the coding parts you need to implement are denoted by:\n",
        "```\n",
        "#######################\n",
        "# Your implementation #\n",
        "#######################\n",
        "```\n",
        "- We always recommend co-operation and discussion in groups for assignments. However, **each student has to finish all the questions by him/herself**. If our matching system identifies any sort of copying, you'll be responsible for consequences.\n",
        "- Students who audit this course should submit their assignments like other students to be qualified for attending the rest of the sessions.\n",
        "- If you have any questions about this assignment, feel free to drop us a line. You may also post your questions on the course Microsoft Teams channel.\n",
        "- You must run this notebook on Google Colab platform, it depends on Google Colab VM for some of the depencecies.\n",
        "- You can double click on collapsed code cells to expand them.\n",
        "- <b><font color='red'>When you are ready to submit, please follow the instructions at the end of this notebook.</font></b>\n",
        "\n",
        "\n",
        "<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLkASPSSL4ep"
      },
      "source": [
        "# Introduction\n",
        "Enough RNNs! Transformers are here!\n",
        "\n",
        "In this assignment, we are going to get a taste of what NLP became after the introduction of Transformers!\n",
        "\n",
        "We will first simply fine-tune a pre-trained transformer-based language model (BERT) on MNLI, so you can compare its performance with your previous assignments. Then we will see even transformers are prone to some kind of errors. Also, we will get familiar with two different methods of interpreting the mysterious nature of transformers. \n",
        "\n",
        "<b><font color='red'>Keep in mind, in this notebook, you have freedome in your implementations.</font></b>\n",
        "\n",
        "Let's begin!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2dukC-zjocR"
      },
      "source": [
        "# Fine-Tuning / Robustness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tICHlFVzyRO"
      },
      "source": [
        "First things first! As an NLP researcher, you should know how to employ a Transformer-based model in a downstream task! A decent choice when it comes to Transformers is the [HuggingFace transformers](https://huggingface.co/transformers/index.html) library. You're free to choose your own tools of course, but we strongly recommend using the mentioned library as it provides a whole new world of utilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mtYzbwM0Vqr"
      },
      "source": [
        "<b><font color=\"seaGreen\">Fine-tune a BERT model on MNLI dataset!</font></b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHxxicGS1yoZ"
      },
      "source": [
        "# prepare dataset\n",
        "\n",
        "#######################\n",
        "# Your implementation #\n",
        "#######################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSvxNrrb1335"
      },
      "source": [
        "# create model architecture\n",
        "\n",
        "#######################\n",
        "# Your implementation #\n",
        "#######################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPiZUcwv16d4"
      },
      "source": [
        "# train your model\n",
        "\n",
        "#######################\n",
        "# Your implementation #\n",
        "#######################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fevk52xo19U5"
      },
      "source": [
        "# evaluate on test set - create a confusion matrix\n",
        "\n",
        "#######################\n",
        "# Your implementation #\n",
        "#######################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64hOIefo2FKJ"
      },
      "source": [
        "<b><font color=\"seaGreen\">Compare your results with the baselines you had in previous assignments! Which class is the hardest for BERT to learn? Why?</font></b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHLdt8OH2cPb"
      },
      "source": [
        "Alright, As you saw, Transformers show a remarkable performance compared to other architectures. Now you might think they have no weaknesses; As we will see, you could not be more wrong! A whole branch of research is devoted to generating adversarial examples to fool models, but here we are interested in \"TextFooler\" which is a simple greedy approach based on replacing the most important tokens with similar ones. Take a look at the [original paper](https://arxiv.org/pdf/1907.11932.pdf). We can find an implementation of `textfooler` in [this repository](https://github.com/QData/TextAttack). Let's install this tool:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYjIdoXqbPrF"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "!pip install textattack\n",
        "\n",
        "clear_output()\n",
        "print(\"Done! Please restart the runtime.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCOK2LYRf0nv"
      },
      "source": [
        "Now let's see some examples of fine-tuned BERT on MNLI getting fooled by simply replacing few words by running the following cell (it may take few minutes)!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEEwYHhocx1w"
      },
      "source": [
        "!textattack attack --model bert-base-uncased-mnli --recipe textfooler --num-examples 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBZUCyBzluck"
      },
      "source": [
        "As you can see, BERT could be easily fooled! One thing to note is that there are both intuitive and counter-intuitive changes among the generated adversarial examples. <b><font color=\"seaGreen\">Find a sample of each and for the intuitive example, explain why you think this way?</font></b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UllWp9p3oqBb"
      },
      "source": [
        "<b><font color=\"red\">Counter-intuitive change:</font></b></br>\n",
        "<font color=\"red\">Premise: <--Write Here--> </font></br>\n",
        "<font color=\"red\">Hypothesis: <--Write Here--> </font></br>\n",
        "<b><font color=\"red\">Changed to: </font></b></br>\n",
        "<font color=\"red\">Premise: <--Write Here--> </font></br>\n",
        "<font color=\"red\">Hypothesis: <--Write Here--> </font></br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkKOlnAvpgYw"
      },
      "source": [
        "<b><font color=\"red\">Intuitive change:</font></b></br>\n",
        "<font color=\"red\">Premise: <--Write Here--> </font></br>\n",
        "<font color=\"red\">Hypothesis: <--Write Here--> </font></br>\n",
        "<b><font color=\"red\">Changed to: </font></b></br>\n",
        "<font color=\"red\">Premise: <--Write Here--> </font></br>\n",
        "<font color=\"red\">Hypothesis: <--Write Here--> </font></br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoO7lvK4pi5w"
      },
      "source": [
        "<b><font color=\"seaGreen\">Why do you think this change is intuitive?</font></b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cq4Hr_4MprQA"
      },
      "source": [
        "\n",
        "<font color=\"red\"> <--Write Your Answer Here--> </font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYHnHkKmp1z5"
      },
      "source": [
        "<b><font color=\"seaGreen\">What do you propose to make BERT more robust to these kind of attacks?</font></b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBZ-KHsMp1z6"
      },
      "source": [
        "\n",
        "<font color=\"red\"> <--Write Your Answer Here--> </font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U81Yf1yvzc0d"
      },
      "source": [
        "# Masked Language Model Interpretation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8g7W0t2zc0e"
      },
      "source": [
        "As the black box models get more complex, *interpratation* and *interpretability* become more important. *Transformers* are a clear example of such models which had many researchs towards explaining their performance. Herein, the method we are going to (implement and) use is **Erasure**. In the [paper](https://arxiv.org/pdf/1612.08220.pdf) that presented this method, it is described as:\n",
        "\n",
        "> *a general methodology to analyze and interpret decisions from a\n",
        "neural model by observing the effects on the model of erasing various parts of the representation, such as input word-vector dimensions, intermediate hidden units, or input words.* "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPz1tLKHzc0f"
      },
      "source": [
        "![MLM](https://www.researchgate.net/publication/340223686/figure/fig1/AS:873545626308609@1585280915468/BERT-Original-sentence-how-are-you-doing-today_W640.jpg)\n",
        "\n",
        "As you know, the well-known transformers that pushed NLP forward (BERT, RoBERTa, GPT2), are lanuage models by nature. So we apply *erasure* to a **Masked Language Model (MLM)** to interpret the model while doing its original task. We limit erasure to input tokens for simplicity. To find out which parts of the input (tokens) have the most impact on the final decision, we need to take these simple steps:\n",
        "\n",
        "1. Prepare a text input with a single mask token. e.g. `My little [MASK] is taller than me.`\n",
        "2. Let our MLM predict the most probable words to fill the mask: `brother, sister, friend, ...`\n",
        "3. Choose one of the most probable words as target word. e.g. `sister`\n",
        "4. Generate perturbed inputs, by erasing context tokens one by one. e.g. `My little [MASK] is [PAD] than me.` You can remove the word or replace it with pad token or ...\n",
        "5. Feed each perturbed input to MLM and measure the change in probability of the target word.\n",
        "6. Finally, the probability change caused by removing each token is considered as the **importance** of that token. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUeJpA8ozc0h"
      },
      "source": [
        "The implementation of above steps is up to you. Obviously, you have no better choice than the widely used `transformers` library, but there are also choices within that:\n",
        "\n",
        "You can use either `*ForMaskedLM` or `FillMaskPipeline` to implement erasure as explained. The former is a transformer `Model` with a language modeling head on top. You can pre-train/fine-tune a `Model` or use it for inference. On the other hand, `Pipeline`s are a great and easy way to use models only for inference. As the name suggests, pipelines run the whole process (including preprocessing) from the input text to the final output, offering ease of use at the \n",
        "cost of inflexibility."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDFVzH5Yzc0i"
      },
      "source": [
        "<b><font color=\"seaGreen\">Implement *erasure* as explained. You are free to break it to multiple functions, but the `compute_importances` function should pack the whole process and return importance scores as described in the docstring. Your implementation should work with any MLM in `transformers`.</font></b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH31PpD0zc0j"
      },
      "source": [
        "# load tokenizer/model/pipeline\n",
        "\n",
        "#######################\n",
        "# Your implementation #\n",
        "#######################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntCc5M1Kzc0k"
      },
      "source": [
        "# fill mask with top-10 (tokens, probs)\n",
        "\n",
        "#######################\n",
        "# Your implementation #\n",
        "#######################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0F3qnLlzc0k"
      },
      "source": [
        "# remove tokens one by one, returns a batch of inputs\n",
        "\n",
        "#######################\n",
        "# Your implementation #\n",
        "#######################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqrOqCu_zc0l"
      },
      "source": [
        "# compute token importances towards predicting a target word\n",
        "def compute_importances(text, target_word_index=0):\n",
        "\"\"\"\n",
        "Computes importance scores for tokens in the input text. The target model is assumed to\n",
        "be gloabl. If you want to change the function signature and pass the model as argument,\n",
        "remember to fix the function calls in the visulization cells bellow.\n",
        "\n",
        "Args:\n",
        "  text: raw text with exactly one mask token\n",
        "  target_word_index: the rank of target word in the top predictions (i-th most probable word)\n",
        "\n",
        "Returns:\n",
        "  tokens: tokenized input text in which the mask token is replaced with target word. (list of str)\n",
        "  importances: list of importance values for each token. Its value for the target token is always zero.\n",
        "\n",
        "\"\"\"\n",
        "  #######################\n",
        "  # Your implementation #\n",
        "  #######################\n",
        "\n",
        "  return tokens, importances"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rm8aSp3Vzc0m",
        "cellView": "form"
      },
      "source": [
        "# @title visualize importance\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.colors as clr\n",
        "from pylab import rcParams\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "def visualize_attention(sentences, score_lists, color_maps='RdYlGn', rtl=False,\n",
        "                        alpha=0.5, font_size=14, token_sep=' ', sentence_sep='<br/><br/>'):\n",
        "\n",
        "  if type(color_maps) is str:\n",
        "    color_maps = [color_maps] * len(sentences)\n",
        "\n",
        "  span_sentences, style_sentences = [], []\n",
        "\n",
        "  for s, tokens in enumerate(sentences):\n",
        "\n",
        "    scores = score_lists[s]\n",
        "    cmap = cm.get_cmap(color_maps[s])\n",
        "    \n",
        "    max_value = max(abs(min(scores)), abs(max(scores)))\n",
        "    normer = clr.Normalize(vmin=-max_value/alpha, vmax=max_value/alpha)\n",
        "    colors = [clr.to_hex(cmap(normer(x))) for x in scores]\n",
        "\n",
        "    if len(tokens) != len(colors):\n",
        "        raise ValueError(\"number of tokens and colors don't match\")\n",
        "\n",
        "    style_elems, span_elems = [], []\n",
        "    for i in range(len(tokens)):\n",
        "        style_elems.append(f'.c{s}-{i} {{ background-color: {colors[i]}; }}')\n",
        "        span_elems.append(f'<span class=\"c{s}-{i}\">{tokens[i]} </span>')\n",
        "\n",
        "    span_sentences.append(token_sep.join(span_elems))\n",
        "    style_sentences.append(' '.join(style_elems))\n",
        "    text_dir = 'rtl' if rtl else 'ltr'\n",
        "\n",
        "  return HTML(f\"\"\"<html><head><link href=\"https://fonts.googleapis.com/css?family=Roboto+Mono&display=swap\" rel=\"stylesheet\">\n",
        "               <style>span {{ font-family: \"Roboto Mono\", monospace; font-size: {font_size}px; padding: 2px}} {' '.join(style_sentences)}</style>\n",
        "               </head><body><p dir=\"{text_dir}\">{sentence_sep.join(span_sentences)}</p></body></html>\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "id": "ZG8sWabkzc0n",
        "cellView": "form",
        "outputId": "bbd4e69e-013d-4880-cfbf-9d2aa68e0da0"
      },
      "source": [
        "#@title implementation verification\n",
        "#@markdown The importance scores for most probable word (` basketball`) in the sentence `My brother plays football, but my sister prefers<mask>.` using  `roberta-base` model should seem like this:\n",
        "#@markdown Uncomment the last two lines and compare your result to ours.\n",
        "\n",
        "\n",
        "# @title visualize importance\n",
        "\n",
        "colormap = \"RdYlGn\" #@param [\"RdYlGn\", \"bwr_r\"]\n",
        "alpha = 0.8 #@param {type:\"number\"}\n",
        "right_to_left = False #@param {type:\"boolean\"}\n",
        "\n",
        "our_tokens = ['My', 'brother', 'plays', 'football', ',', 'but', 'my', 'sister', 'prefers', ' basketball', '.']\n",
        "our_importances = [0.007425040006637573, 0.0023840665817260742, -0.05275896191596985, 0.2751784808933735, 0.032991498708724976, 0.1144493818283081, 0.017473608255386353, 0.06914246082305908, 0.31883253064006567, 0.0, 0.1255783885717392]\n",
        "\n",
        "display(visualize_attention([our_tokens], [our_importances], color_maps=colormap, rtl=right_to_left, alpha=alpha))\n",
        "\n",
        "# your_tokens, your_importances = compute_importances('My brother plays football, but my sister prefers<mask>.', 0)\n",
        "# display(visualize_attention([your_tokens], [your_importances], color_maps=colormap, rtl=right_to_left, alpha=alpha))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html><head><link href=\"https://fonts.googleapis.com/css?family=Roboto+Mono&display=swap\" rel=\"stylesheet\">\n",
              "               <style>span { font-family: \"Roboto Mono\", monospace; font-size: 14px; padding: 2px} .c0-0 { background-color: #fbfdba; } .c0-1 { background-color: #feffbe; } .c0-2 { background-color: #feeb9d; } .c0-3 { background-color: #42ac5a; } .c0-4 { background-color: #eff8aa; } .c0-5 { background-color: #c3e67d; } .c0-6 { background-color: #f7fcb4; } .c0-7 { background-color: #ddf191; } .c0-8 { background-color: #199750; } .c0-9 { background-color: #feffbe; } .c0-10 { background-color: #bbe278; }</style>\n",
              "               </head><body><p dir=\"ltr\"><span class=\"c0-0\">My </span> <span class=\"c0-1\">brother </span> <span class=\"c0-2\">plays </span> <span class=\"c0-3\">football </span> <span class=\"c0-4\">, </span> <span class=\"c0-5\">but </span> <span class=\"c0-6\">my </span> <span class=\"c0-7\">sister </span> <span class=\"c0-8\">prefers </span> <span class=\"c0-9\"> basketball </span> <span class=\"c0-10\">. </span></p></body></html>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U68E3RwBzc0p"
      },
      "source": [
        "Now that you implemented erasure, it is time to interpret our models.\n",
        "\n",
        "<b><font color=\"seaGreen\">Interpret a MLM (of your choice) predictions for two of these three `(text, target word)` provided. Use both positive and negative importance scores in your explanations. The first two texts are from LMBADA dataset, and the third is derived from WinoGrad Schema Challenge. If there is another example which you can explain better (containing more valuable information), replace it with one of the those two. </font></b>\n",
        "\n",
        "### Example #1\n",
        "\n",
        "**text**: `the battery on logan 's radio must have been on the way out . so he told himself . there was no other explanation beyond cygan and the staff at the white house having been overrun . lizzie opened her eyes with a flutter . they had been on the icy road for an hour without incident . jack was happy to do all of the[MASK].`\n",
        "\n",
        "**target word**: `driving`\n",
        "\n",
        "### Example #2\n",
        "\n",
        "**text**: `he heard rhinna speak `` the queen wants you in her carriage . '' tom spoke `` no , i 'm not going in some asylum . '' ran was seen standing next to him spoke `` it 's just for a private talk with you that 's all . '' tom groaned and went inside the carriage to sit down next to the*.`\n",
        "\n",
        "**target word**: `queen`\n",
        "\n",
        "### Example #3\n",
        "**text**: `The delivery truck zoomed by the school bus because the[MASK] was going so fast.`\n",
        "\n",
        "**target word**: `truck`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "FhXBzwKBzc0p"
      },
      "source": [
        "#@title visualize importances 1\n",
        "\n",
        "text = \"the battery on logan 's radio must have been on the way out . so he told himself . there was no other explanation beyond cygan and the staff at the white house having been overrun . lizzie opened her eyes with a flutter . they had been on the icy road for an hour without incident . jack was happy to do all of the[MASK].\" #@param {type:\"string\"}\n",
        "target_token_index = 0 #@param {type:\"integer\"}\n",
        "\n",
        "colormap = \"RdYlGn\" #@param [\"RdYlGn\", \"bwr_r\"]\n",
        "alpha = 0.8 #@param {type:\"number\"}\n",
        "right_to_left = False #@param {type:\"boolean\"}\n",
        "\n",
        "tokens, importances = compute_importances(text, target_token_index)\n",
        "display(visualize_attention([tokens], [importances], color_maps=colormap, rtl=right_to_left, alpha=alpha))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-O3ZMBHqRPA"
      },
      "source": [
        "\n",
        "<font color=\"red\"> <--Write Your Answer Here--> </font>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "EJWS_nO4zc0q"
      },
      "source": [
        "#@title visualize importances 2\n",
        "\n",
        "text = \"The delivery truck zoomed by the school bus because the[MASK] was going so fast.\" #@param {type:\"string\"}\n",
        "target_token_index = 0 #@param {type:\"integer\"}\n",
        "\n",
        "colormap = \"RdYlGn\" #@param [\"RdYlGn\", \"bwr_r\"]\n",
        "alpha = 0.8 #@param {type:\"number\"}\n",
        "right_to_left = False #@param {type:\"boolean\"}\n",
        "\n",
        "tokens, importances = compute_importances(text, target_token_index)\n",
        "display(visualize_attention([tokens], [importances],\n",
        "                            color_maps=colormap, rtl=right_to_left, alpha=alpha))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCZCEniPqS-Q"
      },
      "source": [
        "\n",
        "<font color=\"red\"> <--Write Your Answer Here--> </font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIevkqz6zc0s"
      },
      "source": [
        "<b><font color=\"seaGreen\">Give an example that the importance scores seem counter-intuitive. (You cannot explain that or you could better explain if the scores were reversed!)\n",
        "1.  What do you think about it? Does the model realy work in a way that is not *plausible* to humans? Or the interpretation method is not *faithful*? \n",
        "2. If you doubt the faithfulness of erasure method (our simplified variant), explain its weaknesses in more details.\n",
        "\n",
        "</font></b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "5zLkwXGFzc0s"
      },
      "source": [
        "#@title visualize importances (counter-intuitive)\n",
        "\n",
        "text = \"\" #@param {type:\"string\"}\n",
        "target_token_index = 0 #@param {type:\"integer\"}\n",
        "\n",
        "colormap = \"RdYlGn\" #@param [\"RdYlGn\", \"bwr_r\"]\n",
        "alpha = 0.8 #@param {type:\"number\"}\n",
        "right_to_left = False #@param {type:\"boolean\"}\n",
        "\n",
        "tokens, importances = compute_importances(text, target_token_index)\n",
        "display(visualize_attention([tokens], [importances], color_maps=colormap, rtl=right_to_left, alpha=alpha))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b34lR_TOqvKQ"
      },
      "source": [
        "\n",
        "<font color=\"red\"> <--Write Your Answer Here--> </font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvLeULP3bjbQ"
      },
      "source": [
        "# How do Decisions Emerge across Layers?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zso0qQQq7Wwt"
      },
      "source": [
        "We have implemented a simple interpretation method in the previous part. Erasure is a feature attribution method, i.e. it attributes the prediction to the input features.\n",
        "In the last part of this assignment, we are going to learn about one of the most sophisticated (and most interesting) interpretation methods working on transformers. The method is presented in EMNLP 2020 with the [paper](https://www.aclweb.org/anthology/2020.emnlp-main.262) titled: \n",
        "> **How do Decisions Emerge across Layers in Neural Models?\n",
        "Interpretation with Differentiable Masking**\n",
        "\n",
        "We call the method ***DiffMask***, like the authors did. Thankfully, they release the full source code, so we don't need to implement anything. We just want you to read the paper and deeply understand the concepts, run some code cell and answer some questions. So...\n",
        "\n",
        "<b><font color=\"seaGreen\">Read the paper and answer the following questions. Just go deep enough to answer the questions and explain the visualized results, not deeper! AND IF YOU HAVE INSOLUBLE AMBIGUITY, BE SURE TO CONTACT TAs.\n",
        "\n",
        "1. Why the erasure method is susceptible to hindsight bias?\n",
        "2. What are the advantages of DiffMask over erasure?\n",
        "3. What was the motivation of designing a toy task?\n",
        "4. Figures 5, 6 shows a conflict between the two variants of DiffMask on the importance of special tokens! Can you explain what realy happens for `[CLS]` and `[SEP]` in sentiment anaysis?\n",
        "\n",
        "</font></b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTs3oxhsre38"
      },
      "source": [
        "\n",
        "<font color=\"red\"> <--Write Your Answer Here--> </font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFHzBh7IkbkE"
      },
      "source": [
        "## Prepare environment to run DiffMask for SQuAD\n",
        "\n",
        "Let's run some code and see DiffMask in action."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeE_l7YyYvpW",
        "cellView": "form"
      },
      "source": [
        "#@title install libs (restart after installing)\n",
        "from IPython.display import clear_output\n",
        "\n",
        "!pip install pytorch-lightning==0.7.3 transformers==2.9.0\n",
        "!pip install sty\n",
        "!apt install megatools\n",
        "\n",
        "!git clone https://github.com/nicola-decao/diffmask\n",
        "%cd diffmask\n",
        "!python setup.py install\n",
        "%cd /content\n",
        "\n",
        "clear_output()\n",
        "print('Done! Please reastart runtime.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICD2ziuOg7pE",
        "cellView": "form"
      },
      "source": [
        "#@title download DiffMask model (takse some time)\n",
        "# input squad model\n",
        "!megadl 'https://mega.nz/#!egZTQaLb!Rq6j1KCithdYgxP0aEJAXafPc3_eVS3u8AOjcXQNRF4'\n",
        "\n",
        "# hidden squad model\n",
        "# !megadl 'https://mega.nz/#!GtRWhRwC!FV8ZbqVkiO-MtJQPTBKUTye2g0uwZ75wQdVo2lIBJ04'\n",
        "\n",
        "# sst\n",
        "# !megadl https://mega.nz/#!bxhgnJDQ!ZiZHzoEDPk4Ub5Cj-f9Yey0ekVynqa7AopwZKwQI-X0\n",
        "clear_output()\n",
        "print('Done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNjjtnk7ZA-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "61c43c60-245a-4583-f902-d7dcfa7c70f5"
      },
      "source": [
        "#@title import libs\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import torch\n",
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm, trange\n",
        "\n",
        "from diffmask.models.question_answering_squad_diffmask import (\n",
        "    BertQuestionAnsweringSquadDiffMask,\n",
        ")\n",
        "from diffmask.models.sentiment_classification_sst_diffmask import (\n",
        "    BertSentimentClassificationSSTDiffMask,\n",
        "    RecurrentSentimentClassificationSSTDiffMask,\n",
        "    PerSampleDiffMaskRecurrentSentimentClassificationSSTDiffMask,\n",
        "    PerSampleREINFORCERecurrentSentimentClassificationSSTDiffMask,\n",
        ")\n",
        "from diffmask.utils.plot import plot_sst_attributions\n",
        "from diffmask.utils.plot import plot_squad_attributions, print_attributions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:transformers.file_utils:PyTorch version 1.8.1+cu101 available.\n",
            "INFO:transformers.file_utils:TensorFlow version 2.5.0 available.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eEwGCvBZ3ZA",
        "cellView": "form"
      },
      "source": [
        "#@title load DiffMask model\n",
        "device = \"cuda\"\n",
        "model = BertQuestionAnsweringSquadDiffMask.load_from_checkpoint(\n",
        "    '/content/squad-diffmask-input.ckpt').to(device)\n",
        "model.freeze()\n",
        "\n",
        "clear_output()\n",
        "print('Done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwCOZSO0Sh7p"
      },
      "source": [
        "Now we are ready to go. We have hand-picked a few [SQuAD 1.1](https://rajpurkar.github.io/SQuAD-explorer/explore/1.1/dev/) examples for you to ask some question about them. Of course you can experiment with any arbitrary context, question including those from the SQuAD task. If you fully understand the **Figure 8** in the paper (Visualized DiffMask results for SQuAD), continue and answer the following questions:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzOM6N1jNwlC"
      },
      "source": [
        "<b><font color=\"seaGreen\">Try to explain how the model finds the corrects answer in the following example, using DiffMask results. What is ignored and what is considered important? why?\n",
        "Can you use your insight of the model to make it select \"disruptive students\" by prepending a piece of text to the context? (Try to manually attack the model)</font></b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1EuGepc_peU"
      },
      "source": [
        "question = \"Who may teachers focus on, in order to prioritize attention?\"\n",
        "\n",
        "attack = \"\"\n",
        "\n",
        "context = attack + \"\"\"Where school class sizes are typically 40 to 50 students, maintaining order \\\n",
        "in the classroom can divert the teacher from instruction, leaving little opportunity for \\\n",
        "concentration and focus on what is being taught. In response, teachers may concentrate \\\n",
        "their attention on motivated students, ignoring attention-seeking and disruptive students. \\\n",
        "The result of this is that motivated students, facing demanding university entrance \\\n",
        "examinations, receive disproportionate resources. Given the emphasis on attainment of \\\n",
        "university places, administrators and governors may regard this policy as appropriate.\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eyK9c4fZV4a",
        "cellView": "form"
      },
      "source": [
        "#@title visual output 1\n",
        "\n",
        "inputs_dict = {\n",
        "    k: v.to(device)\n",
        "    for k, v in model.tokenizer.encode_plus(\n",
        "        question,\n",
        "        context,\n",
        "        max_length=384,\n",
        "        pad_to_max_length=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).items()\n",
        "}\n",
        "inputs_dict[\"mask\"] = inputs_dict[\"attention_mask\"]\n",
        "del inputs_dict[\"attention_mask\"]\n",
        "\n",
        "question = model.tokenizer.tokenize(question)\n",
        "context = model.tokenizer.tokenize(context)\n",
        "tokens = [\"[CLS]\"]  + question + [\"[SEP]\"]  + context + [\"[SEP]\"]\n",
        "\n",
        "logits_start_orig, logits_end_orig, expected_L0_full = model.forward_explainer(\n",
        "    **inputs_dict, attribution=True\n",
        ")\n",
        "attributions = expected_L0_full.exp()[0,:len(tokens)].cpu()\n",
        "\n",
        "plot_squad_attributions(attributions, tokens, context, question,\n",
        "                        inputs_dict, logits_start_orig, logits_end_orig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX2dHr-oriCB"
      },
      "source": [
        "\n",
        "<font color=\"red\"> <--Write Your Answer Here--> </font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81X81-fIMCYk"
      },
      "source": [
        "The model gives the wrong answer to the question bellow. (Ground Truth Answers: *the Keraites*, *Keraites*.\n",
        "Prediction: *Jadaran*)\n",
        "\n",
        "<b><font color=\"seaGreen\">According to the visualized results of DiffMask:\n",
        "1. Which parts of the passage causes the model to make mistake?\n",
        "2. Is the model too far from finding the right answer? Can you help it find the answer with minimum change in the passage?\n",
        "\n",
        "</font></b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbILHND4FiOV"
      },
      "source": [
        "question = \"What tribe did Toghrul lead?\"\n",
        "\n",
        "context = \"\"\"Temüjin began his ascent to power by offering himself as an ally (or, \\\n",
        "according to other sources, a vassal) to his father's anda (sworn brother or blood \\\n",
        "brother) Toghrul, who was Khan of the Keraites, and is better known by the Chinese \\\n",
        "title \"Wang Khan\", which the Jurchen Jin dynasty granted him in 1197. This relationship \\\n",
        "was first reinforced when Börte was captured by the Merkits. Temüjin turned to Toghrul \\\n",
        "for support, and in response, Toghrul offered his vassal 20,000 of his Keraite warriors \\\n",
        "and suggested that he also involve his childhood friend Jamukha, who had himself become \\\n",
        "Khan (ruler) of his own tribe, the Jadaran.\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "8hT1c2zH_sOW"
      },
      "source": [
        "#@title visual output 2\n",
        "\n",
        "inputs_dict = {\n",
        "    k: v.to(device)\n",
        "    for k, v in model.tokenizer.encode_plus(\n",
        "        question,\n",
        "        context,\n",
        "        max_length=384,\n",
        "        pad_to_max_length=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).items()\n",
        "}\n",
        "inputs_dict[\"mask\"] = inputs_dict[\"attention_mask\"]\n",
        "del inputs_dict[\"attention_mask\"]\n",
        "\n",
        "question = model.tokenizer.tokenize(question)\n",
        "context = model.tokenizer.tokenize(context)\n",
        "tokens = [\"[CLS]\"]  + question + [\"[SEP]\"]  + context + [\"[SEP]\"]\n",
        "\n",
        "logits_start_orig, logits_end_orig, expected_L0_full = model.forward_explainer(\n",
        "    **inputs_dict, attribution=True\n",
        ")\n",
        "attributions = expected_L0_full.exp()[0,:len(tokens)].cpu()\n",
        "\n",
        "plot_squad_attributions(attributions, tokens, context, question,\n",
        "                        inputs_dict, logits_start_orig, logits_end_orig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VewnWnrGrirv"
      },
      "source": [
        "\n",
        "<font color=\"red\"> <--Write Your Answer Here--> </font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGcRhj68Hcca"
      },
      "source": [
        "<b><font color=\"seaGreen\">How can you explain the result of DiffMask for the example below? (the answer is correct) What does it mean when some tokens are considered removable in the middle layers, but are selected as the final answer?</font></b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZAAbpqgGf7e"
      },
      "source": [
        "question = \"What is the mlolongo system?\"\n",
        "\n",
        "context = \"\"\"The election held in 1988 saw the advent of the mlolongo (queuing) system, \\\n",
        "where voters were supposed to line up behind their favoured candidates instead of a secret \\\n",
        "ballot. This was seen as the climax of a very undemocratic regime and it led to widespread \\\n",
        "agitation for constitutional reform. Several contentious clauses, including one that \\\n",
        "allowed for only one political party were changed in the following years. In democratic, \\\n",
        "multiparty elections in 1992 and 1997, Daniel arap Moi won re-election.\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "e-J330-AG-yh"
      },
      "source": [
        "#@title visual output 3\n",
        "\n",
        "inputs_dict = {\n",
        "    k: v.to(device)\n",
        "    for k, v in model.tokenizer.encode_plus(\n",
        "        question,\n",
        "        context,\n",
        "        max_length=384,\n",
        "        pad_to_max_length=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).items()\n",
        "}\n",
        "inputs_dict[\"mask\"] = inputs_dict[\"attention_mask\"]\n",
        "del inputs_dict[\"attention_mask\"]\n",
        "\n",
        "question = model.tokenizer.tokenize(question)\n",
        "context = model.tokenizer.tokenize(context)\n",
        "tokens = [\"[CLS]\"]  + question + [\"[SEP]\"]  + context + [\"[SEP]\"]\n",
        "\n",
        "logits_start_orig, logits_end_orig, expected_L0_full = model.forward_explainer(\n",
        "    **inputs_dict, attribution=True\n",
        ")\n",
        "attributions = expected_L0_full.exp()[0,:len(tokens)].cpu()\n",
        "\n",
        "plot_squad_attributions(attributions, tokens, context, question,\n",
        "                        inputs_dict, logits_start_orig, logits_end_orig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l98WMU5brjLQ"
      },
      "source": [
        "\n",
        "<font color=\"red\"> <--Write Your Answer Here--> </font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWRra63i2oFS"
      },
      "source": [
        "# Submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQ6iRsAn2rlE"
      },
      "source": [
        "Congratulations! You finished the assignment & you're ready to submit your work. Please follow the instructions:\n",
        "\n",
        "1. Check and review your answers. Make sure all of the cell outputs are what you want. \n",
        "2. Select File > Save.\n",
        "3. **Fill your information** & run the cell bellow.\n",
        "4. Run **Make Submission** cell, It may take several minutes and it may ask you for your credential.\n",
        "5. Run **Download Submission** cell to obtain your submission as a zip file.\n",
        "6. Grab the downloaded file (`nlp_asg04__xx__xx.zip`) and hand it over in microsoft teams."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lL9OYI1C1wRq"
      },
      "source": [
        "## Fill your information (Run the cell)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ra5wTxj62CWc",
        "cellView": "form"
      },
      "source": [
        "#@title Enter your information & \"RUN the cell!!\" { run: \"auto\" }\n",
        "student_id = \"\" #@param {type:\"string\"}\n",
        "student_name = \"\" #@param {type:\"string\"}\n",
        "\n",
        "print(\"your student id:\", student_id)\n",
        "print(\"your name:\", student_name)\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "ASSIGNMENT_PATH = Path('asg04')\n",
        "ASSIGNMENT_PATH.mkdir(parents=True, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFYJJJhh3kpj"
      },
      "source": [
        "## Make Submission (Run the cell)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBQc5tBQ2sFJ",
        "cellView": "form"
      },
      "source": [
        "#@title Make submission\n",
        "! pip install -U --quiet PyDrive > /dev/null\n",
        "! pip install -U --quiet jdatetime > /dev/null\n",
        "\n",
        "# ! wget -q https://github.com/github/hub/releases/download/v2.10.0/hub-linux-amd64-2.10.0.tgz \n",
        "\n",
        "\n",
        "import os\n",
        "import time\n",
        "import yaml\n",
        "import json\n",
        "import jdatetime\n",
        "\n",
        "from google.colab import files\n",
        "from IPython.display import Javascript\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "asg_name = 'NLP_Assignment_4'\n",
        "script_save = '''\n",
        "require([\"base/js/namespace\"],function(Jupyter) {\n",
        "    Jupyter.notebook.save_checkpoint();\n",
        "});\n",
        "'''\n",
        "# repo_name = 'iust-deep-learning-assignments'\n",
        "submission_file_name = 'nlp_asg04__%s__%s.zip'%(student_id, student_name.lower().replace(' ',  '_'))\n",
        "\n",
        "sub_info = {\n",
        "    'student_id': student_id,\n",
        "    'student_name': student_name, \n",
        "    'dateime': str(jdatetime.date.today()),\n",
        "    'asg_name': asg_name\n",
        "}\n",
        "json.dump(sub_info, open('info.json', 'w'))\n",
        "\n",
        "Javascript(script_save)\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "file_id = drive.ListFile({'q':\"title='%s.ipynb'\"%asg_name}).GetList()[0]['id']\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('%s.ipynb'%asg_name) \n",
        "\n",
        "! jupyter nbconvert --to script \"$asg_name\".ipynb > /dev/null\n",
        "! jupyter nbconvert --to html \"$asg_name\".ipynb > /dev/null\n",
        "! zip \"$submission_file_name\" \"$asg_name\".ipynb \"$asg_name\".html \"$asg_name\".txt info.json > /dev/null\n",
        "\n",
        "print(\"##########################################\")\n",
        "print(\"Done! Submisson created, Please download using the bellow cell!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3n3JS51xqUo"
      },
      "source": [
        "drive.ListFile({'q':\"title='%s.ipynb'\"%asg_name}).GetList()[0]['id']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RclPk2VM30Qa"
      },
      "source": [
        "files.download(submission_file_name)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}