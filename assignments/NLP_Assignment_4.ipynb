{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment_4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 4\n",
        "\n",
        "In this homework, we will practise using the **ðŸ¤— Hugging Face** libraries for transformers and datasets for natural language processing. Follow the directions in each section to complete the expected parts. Take into account the following guidelines:\n",
        "\n",
        "*   **Assignment Due Date:** <b><font color='red'>1401.06.xx</font></b> 23:59:00\n",
        "*   We always recommend co-operation and discussion in groups for assignments. However, each student has to finish all the questions by him/herself. If our matching system identifies any sort of copying, you'll be responsible for consequences.\n",
        "\n",
        "*   The items you need to answer are highlighted in bold SeaGreen and the coding parts you need to implement are denoted by:\n",
        "```\n",
        "#######################\n",
        "# Your implementation #\n",
        "#######################\n",
        "```\n",
        "*   If you have any issues about this assignment, please do not hesitate to contact us. But before, check the provided documentation links for Hugging Face and PyTorch first.\n",
        "*   This code requires some of the dependencies of the COLAB platform, you are free to use any other platform that supports jupyter notebooks, but there is no guarantee that the code snippets will work in your setting without some modifications. \n",
        "*   You can double click on collapsed code cells to expand them.\n",
        "*   <b><font color='red'>When you are ready to submit, please follow the instructions at the end of this notebook.</font></b>\n",
        "\n"
      ],
      "metadata": {
        "id": "7LcuwBXtMEn9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkeTVdUW6Cgu",
        "outputId": "8e3b866f-5f38-40ee-eed4-425bf27a267d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done!\n"
          ]
        }
      ],
      "source": [
        "#@title Import and Install Essential Packages\n",
        "\n",
        "%pip install transformers tokenizers datasets\n",
        "\n",
        "# Enable the following line if you want to see error stack in GPU mode\n",
        "# %env CUDA_LAUNCH_BLOCKING=1\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "clear_output()\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Advanced Interpretability\n",
        "\n",
        "In the previous homework, we had the opportunity to work with Erasure, a simple and effective interpretabation method that works by erasing the tokens one by one and observing the change in probability of a selected label. We also had an early peek to more advanced forms of interpretation. In this section, we are going to work with some of the newer forms of saliency and discuss their advantages and disadvantages. \n",
        "\n",
        "The good news is that you are not required to train any models in this section as we will provide the required model to you. \n",
        "\n",
        "Lets start by implementing and analyzing a form of interpretation called attention based saliency or attention based interpretation. "
      ],
      "metadata": {
        "id": "DKFdjtHBQaM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1.1 Attention Based Interpretation\n",
        "\n",
        "As you already know, Transformer models rely heavily on attention mechanism to make their inferences. More specifically, they make use of a mechanism called self-attention, in which the final representation is created by looking at the \"importance\" of other tokens in the same sentence. This final representation is then used for classification. \n",
        "\n",
        "One way to interpret the behavior of our model is to make use of these attention scores, with higher attention score meaning that the model priorities the token attributed to that attention score. \n",
        "\n",
        "To start things off, run the cell below to load a multilingual sentiment analysis bert model. We already configure this model to ease your work. "
      ],
      "metadata": {
        "id": "4zdnCfXsaPyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
        "Config = BertConfig.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "tokenizer = BertTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "model = BertForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\", config = Config)"
      ],
      "metadata": {
        "id": "orSmqk0F6I9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To give an explanation of the loaded model, we first briefly describe what it does. \n",
        "\n",
        "This is a classic Sentiment Analysis model in which an input is classified to a class labeled 0 to 5. With 0 meaning extremely negative, and 5 meaning extremely positive. \n",
        "\n",
        "Complete the function below such that given a model, a tokenizer, and a label, the output is returned containing the loss, logits, and attentions. Note that you have to perform an extra step for the model to return the attentions."
      ],
      "metadata": {
        "id": "PEEvskZojT3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def return_output(model, tokenizer, raw_text, label) : \n",
        "  outputs = None\n",
        "  ####################\n",
        "  #Implement Your Code Here\n",
        "  ####################\n",
        "  return outputs\n",
        "print(return_output(model = model,tokenizer = tokenizer, raw_text = 'i love books', label = 4))"
      ],
      "metadata": {
        "id": "7p8OU47VjTdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great, now that you have your outputs and attentions, answer the questions below. \n",
        "\n",
        "1 - Note that the attentions returned by the model have a length of $X$, first, specify what number is $X$, then explain what it shows. \n",
        "\n",
        "2 - Note that each element of attentions itself has a length of $Y$, first, specify what number is $Y$, then explain what it shows. \n",
        "\n",
        "3 - Note that the length of the final attention scores is higher than the length of our sentence. Why do you think this happens? Is this length always higher than the length of the provided sentence? "
      ],
      "metadata": {
        "id": "XnFAxKhsmCC5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*** Type Your Answer Here ***"
      ],
      "metadata": {
        "id": "ImDcsYz2oAhn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great, now that you have a understanding of the attentions returned by the model, it is time to calculate the saliency for each input. To ease your work, we are going to provide a simple algorithm which can you can follow. \n",
        "\n",
        "1 - Get the output of a sentence from the function you have written above. \n",
        "\n",
        "2 - Retrieve the attentions from the final embedding of the model. \n",
        "\n",
        "3 - Retrieve the attentions from every attention head of the final embedding.\n",
        "\n",
        "4 - Look at the attention scores with respect to the CLS token, we will use the attentions of this token to calculate our final scores. \n",
        "\n",
        "5 - For every attention head, find the scores of each token with respect to the CLS token. \n",
        "\n",
        "6 - Average across attention heads to get the attention score for each token.\n",
        "\n",
        "Having this information, complete the function below such that given an output instance and an input text, attention scores are calculated for each token. "
      ],
      "metadata": {
        "id": "xiGHNnL6o1-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_attention_scores(outputs, raw_text) : \n",
        "  '''\n",
        "  outputs: an output instance received from the model\n",
        "  raw_text: raw text that is fed into the model \n",
        "\n",
        "  returns : \n",
        "  attention_scores: A list of scores, the same length as the list of tokens \n",
        "  list_of_tokens = A list of tokens, tokenized\n",
        "  '''\n",
        "  ##############\n",
        "  #Implement Your Code Here\n",
        "  ##############\n",
        "  return attention_scores, list_of_tokens"
      ],
      "metadata": {
        "id": "_fBGcv_NjQgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great job, if you have managed to get the attention scores, it means that you have a deep understanding of transformer models. \n",
        "\n",
        "Now, as is our tradition, it is time to visualize the output. Run the code below to prepare visualization."
      ],
      "metadata": {
        "id": "VlCHVRMZwk5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Visualize importance\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.colors as clr\n",
        "from pylab import rcParams\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "def visualize_attention(sentences, score_lists, color_maps='RdYlGn', rtl=False,\n",
        "                        alpha=0.5, font_size=14, token_sep=' ', sentence_sep='<br/><br/>'):\n",
        "\n",
        "  if type(color_maps) is str:\n",
        "    color_maps = [color_maps] * len(sentences)\n",
        "\n",
        "  span_sentences, style_sentences = [], []\n",
        "\n",
        "  for s, tokens in enumerate(sentences):\n",
        "\n",
        "    scores = score_lists[s]\n",
        "    cmap = cm.get_cmap(color_maps[s])\n",
        "    \n",
        "    max_value = max(abs(min(scores)), abs(max(scores)))\n",
        "    normer = clr.Normalize(vmin=-max_value/alpha, vmax=max_value/alpha)\n",
        "    colors = [clr.to_hex(cmap(normer(x))) for x in scores]\n",
        "\n",
        "    if len(tokens) != len(colors):\n",
        "        raise ValueError(\"number of tokens and colors don't match\")\n",
        "\n",
        "    style_elems, span_elems = [], []\n",
        "    for i in range(len(tokens)):\n",
        "        style_elems.append(f'.c{s}-{i} {{ background-color: {colors[i]}; }}')\n",
        "        span_elems.append(f'<span class=\"c{s}-{i}\">{tokens[i]} </span>')\n",
        "\n",
        "    span_sentences.append(token_sep.join(span_elems))\n",
        "    style_sentences.append(' '.join(style_elems))\n",
        "    text_dir = 'rtl' if rtl else 'ltr'\n",
        "\n",
        "  return HTML(f\"\"\"<html><head><link href=\"https://fonts.googleapis.com/css?family=Roboto+Mono&display=swap\" rel=\"stylesheet\">\n",
        "               <style>span {{ font-family: \"Roboto Mono\", monospace; font-size: {font_size}px; padding: 2px}} {' '.join(style_sentences)}</style>\n",
        "               </head><body><p dir=\"{text_dir}\">{sentence_sep.join(span_sentences)}</p></body></html>\"\"\")"
      ],
      "metadata": {
        "id": "pkRBEDJ0w2lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that have our visualization function ready, lets test our method. Run the cell below to visualize a pre-processed example that we have made for you.  "
      ],
      "metadata": {
        "id": "6-zlNOZ-xOZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualize importances\n",
        "\n",
        "raw_text = \"i love books\" #@param {type:\"string\"}\n",
        "\n",
        "colormap = \"bwr_r\" #@param [\"RdYlGn\", \"bwr_r\"]\n",
        "alpha = 0.8 #@param {type:\"number\"}\n",
        "right_to_left = False #@param {type:\"boolean\"}\n",
        "\n",
        "display(visualize_attention([['[CLS]', 'i' ,'love', 'books', '[SEP]']], [[0.06659328, 0.15865228, 0.31966418, 0.04583241, 0.40925785]], color_maps=colormap, rtl=right_to_left, alpha=alpha))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "id": "OJlZHJ3nxeDY",
        "outputId": "c460e5f8-4168-4067-aacf-eebc12bbea2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<html><head><link href=\"https://fonts.googleapis.com/css?family=Roboto+Mono&display=swap\" rel=\"stylesheet\">\n",
              "               <style>span { font-family: \"Roboto Mono\", monospace; font-size: 14px; padding: 2px} .c0-0 { background-color: #dedeff; } .c0-1 { background-color: #b0b0ff; } .c0-2 { background-color: #6060ff; } .c0-3 { background-color: #e8e8ff; } .c0-4 { background-color: #3232ff; }</style>\n",
              "               </head><body><p dir=\"ltr\"><span class=\"c0-0\">[CLS] </span> <span class=\"c0-1\">i </span> <span class=\"c0-2\">love </span> <span class=\"c0-3\">books </span> <span class=\"c0-4\">[SEP] </span></p></body></html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great. Now, in order to test your code, run the cell below. The output must be very close to the output of the above code (As we are running the same model). \n",
        "\n",
        "Note that if you get a vastly different result, there might be a mistake in your code (or in ours). "
      ],
      "metadata": {
        "id": "dfdmK-lnybTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attention_scores, tokens = calculate_attention_scores(outputs, 'i love books')\n",
        "colormap = \"bwr_r\" #@param [\"RdYlGn\", \"bwr_r\"]\n",
        "alpha = 0.8 #@param {type:\"number\"}\n",
        "right_to_left = False #@param {type:\"boolean\"}\n",
        "\n",
        "display(visualize_attention([tokens], [attention_scores], color_maps=colormap, rtl=right_to_left, alpha=alpha))"
      ],
      "metadata": {
        "id": "kbjxfpcNyy_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congratulations, you have successfully deployed a simple attention based interpretation method.\n",
        "\n",
        "You should have a fairly deep understanding of transformer models by now. To test your knowledge, answer the questions below. \n",
        "\n",
        "1 - Take into consideration the task that our model performs, what kind of words does the model usually attend to when making decisions? Try a few examples and explain why this is the case. \n",
        "\n",
        "2 - Note that the [SEP] token is heavily attended to despite not being in the sentence in many cases. Hypothesize why this is the case. Explain your hypothesis. \n",
        "\n",
        "3 - Compare our new method to the previous method of erasure, what are the differences? What are the advantages of this new approach? Can you think of any disadvantages too? "
      ],
      "metadata": {
        "id": "NTnliJaezlA5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*** Type Your Answer Here *** "
      ],
      "metadata": {
        "id": "UehC6EZb0b3y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Does Pre-trained Language Models Understand Syntatic Information?\n",
        "\n",
        "In this section, we want to see if language models trained on a big corpus can tell us anything about a language's syntax. To do this, we used a POS tagging dataset to determine if our model can understand the POS tags of distinct words in phrases.\n",
        "\n",
        "First, run the following cells to download the dataset and corresponding labels, and import essential libraries.Â \n"
      ],
      "metadata": {
        "id": "tssqI1SL8t0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import the Libraries\n",
        "\n",
        "%pip install transformers\n",
        "\n",
        "from IPython.display import clear_output\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "torch.manual_seed(1)\n",
        "clear_output()\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "WCB1AOAOBM38",
        "outputId": "ce63ea4d-1780-42d0-a87f-10c3e44facd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download and Read the Dataset\n",
        "\n",
        "!gdown 1-SF1kN-Ojv2XyLL8OwaO8CGmZVRT9NPK\n",
        "!unzip dataset.zip\n",
        "\n",
        "import os\n",
        "\n",
        "PREFIX_PATH = '/content/dataset/en-ud-'\n",
        "\n",
        "with open(PREFIX_PATH + \"train.txt\") as f:\n",
        "    train_sentences = [line.strip().split() for line in f.readlines()]\n",
        "with open(PREFIX_PATH + \"test.txt\") as f:\n",
        "    test_sentences = [line.strip().split() for line in f.readlines()]\n",
        "\n",
        "with open(PREFIX_PATH + \"train.pos\") as f:\n",
        "    train_labels = [line.strip().split() for line in f.readlines()]\n",
        "with open(PREFIX_PATH + \"test.pos\") as f:\n",
        "    test_labels = [line.strip().split() for line in f.readlines()]\n",
        "with open(PREFIX_PATH + \"dev.pos\") as f:\n",
        "    dev_labels = [line.strip().split() for line in f.readlines()]\n",
        "\n",
        "# take a fraction of the data\n",
        "train_sentences = train_sentences[:round(len(train_sentences)*0.1)]\n",
        "test_sentences = train_sentences[:round(len(test_sentences)*0.1)]\n",
        "train_labels = train_labels[:round(len(train_labels)*0.1)]\n",
        "test_labels = train_labels[:round(len(test_labels)*0.1)]\n",
        "\n",
        "unique_labels = list(set.union(*[set(l) for l in train_labels + test_labels + dev_labels]))\n",
        "label2index = dict()\n",
        "for label in unique_labels:\n",
        "    label2index[label] = label2index.get(label, len(label2index))\n",
        "\n",
        "train_labels = [[label2index[l] for l in labels] for labels in train_labels]\n",
        "test_labels = [[label2index[l] for l in labels] for labels in test_labels]\n",
        "\n",
        "clear_output()\n",
        "print(f\"unique_labels: {unique_labels}\")\n",
        "print(f\"train_sentences[0]: {train_sentences[0]}\")\n",
        "print(f\"train_labels[0]: {train_labels[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBHWGtK38-O4",
        "outputId": "7fda1f43-7e61-4505-d576-0a161beb8455",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unique_labels: ['SYM', 'PROPN', 'ADJ', 'VERB', 'NUM', 'DET', 'PART', 'ADV', 'PRON', 'INTJ', 'PUNCT', 'CCONJ', 'AUX', 'ADP', 'SCONJ', 'NOUN', 'X']\n",
            "train_sentences[0]: ['Al', '-', 'Zaman', ':', 'American', 'forces', 'killed', 'Shaikh', 'Abdullah', 'al', '-', 'Ani', ',', 'the', 'preacher', 'at', 'the', 'mosque', 'in', 'the', 'town', 'of', 'Qaim', ',', 'near', 'the', 'Syrian', 'border', '.']\n",
            "train_labels[0]: [1, 10, 1, 10, 2, 15, 3, 1, 1, 1, 10, 1, 10, 5, 15, 13, 5, 15, 13, 5, 15, 13, 1, 10, 13, 5, 2, 15, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2.1: Evaluate Quality of Representations for POS Tagging"
      ],
      "metadata": {
        "id": "u64oebglKgXs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to accomplish this, we choose a dataset and a linguistic property to investigate in this experiment. A probing model also requires an auxiliary classifier. A simple linear classifier takes a word representation and transforms it to the label space. We will also require a pre-trained language model to study, which will be roberta-based.\n",
        "\n",
        "**Note:** To begin, use your favourite deep learning framework to initialise the model, tokenizer, loss function, and optimizer. Keep in mind that your classifier only requires a single linear transformation layer."
      ],
      "metadata": {
        "id": "u63sPU7-CZwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create the Classifier\n",
        "\n",
        "model_name = 'roberta-base'  #@param {type:\"string\"}\n",
        "\n",
        "model =                     # Your implementation\n",
        "tokenizer =                 # Your implementation\n",
        "\n",
        "embedding_dim =             # Your implementation\n",
        "num_labels = len(label2index)\n",
        "\n",
        "# define classifier's architecture\n",
        "\n",
        "#######################\n",
        "# Your implementation #\n",
        "#######################\n",
        "\n",
        "def build_classifier(embedding_dim, num_labels):\n",
        "    classifier =            # Your implementation\n",
        "    criterion =             # Your implementation\n",
        "    optimizer =             # Your implementation\n",
        "    return classifier, criterion, optimizer\n",
        "\n",
        "# build classifier\n",
        "classifier, criterion, optimizer = build_classifier(embedding_dim, num_labels, device)\n",
        "\n",
        "print(classifier)"
      ],
      "metadata": {
        "id": "UPEZKF0fBwh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the prepaired language model and tokenizer in hand, your objective in this section is to tokenize and encode sentences. Remember that each word only requires one representation, even if some words contain several subtokens. Take the representation of the final subtoken in each word to solve this problem.\n",
        "\n",
        "**Note:** To implement this section, utilise `word_ids` in the result object when calling `PretrainedTokenizerFast`."
      ],
      "metadata": {
        "id": "NHAAMypNjnFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Tokenize Sentences\n",
        "\n",
        "#######################\n",
        "# Your implementation #\n",
        "#######################\n",
        "\n",
        "print(text_sentences[0], text_representations[0])"
      ],
      "metadata": {
        "id": "d_Tjthgtjm54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will train the classifier on the linguistic annotations now that we have everything we need to run our experiment. In this section, you must finish the training loop using the representations that you prepaired in the previous phase. Remember to only train the classification head and not the weights of the language model itself."
      ],
      "metadata": {
        "id": "U_u8K3SKd58C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Training Loop\n",
        "\n",
        "def train(classifier, criterion, optimizer, inputs, labels, num_epochs=2):\n",
        "  epoch_pbar = tqdm(range(num_epochs))\n",
        "  for epoch in epoch_pbar:\n",
        "\n",
        "    #######################\n",
        "    # Your implementation #\n",
        "    #######################\n",
        "\n",
        "  epoch_pbar.set_postfix(total_loss=total_loss, average_loss=total_loss/total_count)\n",
        "\n",
        "\n",
        "  return avarage_loss, accuracy\n",
        "\n",
        "train(classifier, criterion, optimizer, text_representations, )"
      ],
      "metadata": {
        "id": "r7-p1iZRjlzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you have the classifier ready, you need to evaluate the quality of representations for POS taging. To implement this part, you need to generate representations for tokens using the pre-trained model (i.e., the last hidden state). Use the generated representatinos to find the accuracy of the classifier to classify POS tags. In this experiment, we use the accuracy on this task as a measure of how well language models understand words' POS tags."
      ],
      "metadata": {
        "id": "Bqh5Ab9eGdXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yBX78SFJGdF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q: Analyze the accuracy of your classifier. Can you identify whether the pre-trained model recognises POS tags?\n",
        "\n",
        "*** Type Your Answer Here *** "
      ],
      "metadata": {
        "id": "e5rhwTttLq8g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2.2: Performance of Different Layers\n",
        "\n",
        "One of the major questions in neural network interpretability is how information is organized in different parts of the deep model, such as its layers. Utilize your implementations in previous sections to train different classifiers on each layer and evaluate those classifiers on the test set."
      ],
      "metadata": {
        "id": "zJKbEbYZM4Su"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DXK6FszxMAT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q: Which layer has the best representations for classifying POS tags? Explain why do you think this layer works better for this specific task? Do deeper layers' representatinos work best in all tasks?\n",
        "\n",
        "*** Type Your Answer Here *** "
      ],
      "metadata": {
        "id": "Zau-IknkNWk_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2.3: Dependency on Classifier's Architecture\n",
        "\n",
        "Do you think that the probing accuracy is dependent on the probing model? Create a simple experiment to show your explanations. "
      ],
      "metadata": {
        "id": "nsF7qhPrN2V0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TTppzcFuNY1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain your experiment and setup and justify your method.\n",
        "\n",
        "*** Type Your Answer Here *** "
      ],
      "metadata": {
        "id": "j_-luTA1nyt-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2.4: Control Task\n",
        "\n",
        "In this experiment, we find out how much of the performance in previous experiments was due to the syntactic information learned by the model and how much was due to the probe classifier. We use a method suggested by Hewitt and Liang (https://arxiv.org/pdf/1909.03368.pdf), in which we make a control task that has nothing to do with the POS tagging task and do the same thing on it. Then, we measure the layers' selectivity, which is the difference between how well they worked on the POS task and how well they worked on the control task. If a layer has learned a lot about the POS task in particular, it should be much better at the POS task than the control task; that is, it should have high selectivity. \n",
        "\n",
        "What you should implement:\n",
        "\n",
        "1. Make a new dataset that is similar to the original POS tagging dataset, except that each word is given a random POS tag and the tags are still spread out based on how often they were used in the original dataset. \n",
        "2. Run the same experiments in previous sections on the control task dataset.\n",
        "3. Report selectivity for each layer of BERT on POS tagging dataset.\n",
        "4. For more information you can read the Hewitt and Liang paper.\n",
        "\n"
      ],
      "metadata": {
        "id": "UkXB3JT9n8iQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Cuzo2ugnn1Dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q: Why is selectivity a better way to measure how much knowledge a language model stores? Compare each layer's selectivity to the accuracy of the original probe classifier.\n",
        "\n",
        "*** Type Your Answer Here ***"
      ],
      "metadata": {
        "id": "PB3jh11QuLlt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lL9OYI1C1wRq"
      },
      "source": [
        "## Fill your information (Run the cell)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ra5wTxj62CWc",
        "cellView": "form"
      },
      "source": [
        "#@title Enter your information & \"RUN the cell!!\" { run: \"auto\" }\n",
        "student_id = \"\" #@param {type:\"string\"}\n",
        "student_name = \"\" #@param {type:\"string\"}\n",
        "\n",
        "print(\"your student id:\", student_id)\n",
        "print(\"your name:\", student_name)\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "ASSIGNMENT_PATH = Path('asg01')\n",
        "ASSIGNMENT_PATH.mkdir(parents=True, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFYJJJhh3kpj"
      },
      "source": [
        "## Make Submission (Run the cell)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBQc5tBQ2sFJ",
        "cellView": "form"
      },
      "source": [
        "#@title Make submission\n",
        "! pip install -U --quiet PyDrive > /dev/null\n",
        "! pip install -U --quiet jdatetime > /dev/null\n",
        "\n",
        "# ! wget -q https://github.com/github/hub/releases/download/v2.10.0/hub-linux-amd64-2.10.0.tgz \n",
        "\n",
        "\n",
        "import os\n",
        "import time\n",
        "import yaml\n",
        "import json\n",
        "import jdatetime\n",
        "\n",
        "from google.colab import files\n",
        "from IPython.display import Javascript\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "asg_name = 'NLP_Assignment_4'\n",
        "script_save = '''\n",
        "require([\"base/js/namespace\"],function(Jupyter) {\n",
        "    Jupyter.notebook.save_checkpoint();\n",
        "});\n",
        "'''\n",
        "# repo_name = 'iust-deep-learning-assignments'\n",
        "submission_file_name = 'dl_asg01__%s__%s.zip'%(student_id, student_name.lower().replace(' ',  '_'))\n",
        "\n",
        "sub_info = {\n",
        "    'student_id': student_id,\n",
        "    'student_name': student_name, \n",
        "    'dateime': str(jdatetime.date.today()),\n",
        "    'asg_name': asg_name\n",
        "}\n",
        "json.dump(sub_info, open('info.json', 'w'))\n",
        "\n",
        "Javascript(script_save)\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "file_id = drive.ListFile({'q':\"title='%s.ipynb'\"%asg_name}).GetList()[0]['id']\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('%s.ipynb'%asg_name) \n",
        "\n",
        "! jupyter nbconvert --to script \"$asg_name\".ipynb > /dev/null\n",
        "! jupyter nbconvert --to html \"$asg_name\".ipynb > /dev/null\n",
        "! zip \"$submission_file_name\" \"$asg_name\".ipynb \"$asg_name\".html \"$asg_name\".txt info.json > /dev/null\n",
        "\n",
        "print(\"##########################################\")\n",
        "print(\"Done! Submisson created, Please download using the bellow cell!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3n3JS51xqUo"
      },
      "source": [
        "drive.ListFile({'q':\"title='%s.ipynb'\"%asg_name}).GetList()[0]['id']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RclPk2VM30Qa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4646966a-3c62-4dec-eae5-4b4027006e4c"
      },
      "source": [
        "files.download(submission_file_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_6851c31e-7ef2-463e-8397-80ba802c2cb4\", \"dl_asg01__3991305965005__mahdi_zakizadeh.zip\", 144046)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}