{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a13ec92f6fd04006b0c35f84f16dfe7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_59311c7c313b4eda91ef6bbb01fd39d3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fd4e945ebf3941ad831febb248b6d44f",
              "IPY_MODEL_4dd2f5ea8fd14cf0b6a336d3a2fdcdb0"
            ]
          }
        },
        "59311c7c313b4eda91ef6bbb01fd39d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fd4e945ebf3941ad831febb248b6d44f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c83a6bfa67f443d08addf471b2651ead",
            "_dom_classes": [],
            "description": "Downloading: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1877,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1877,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3635577b6ab94b838efd7d4cab4a27ec"
          }
        },
        "4dd2f5ea8fd14cf0b6a336d3a2fdcdb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_39d6d7882798403ba8a39e3170de46ef",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5.03k/? [00:01&lt;00:00, 4.66kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_da4d7885012145969732add4e27a6efa"
          }
        },
        "c83a6bfa67f443d08addf471b2651ead": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3635577b6ab94b838efd7d4cab4a27ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "39d6d7882798403ba8a39e3170de46ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "da4d7885012145969732add4e27a6efa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "61fd1687e0674e099b33354bdead4317": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8e2b3dda0e114fe9b7127ca2ceeb35a3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c587a678e48541cf8eb5b9d412c0fda4",
              "IPY_MODEL_b7e63c909ce8403499ce4031be39cf96"
            ]
          }
        },
        "8e2b3dda0e114fe9b7127ca2ceeb35a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c587a678e48541cf8eb5b9d412c0fda4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ddaad7ef3fd94f61bb9db437870987d8",
            "_dom_classes": [],
            "description": "Downloading: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 955,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 955,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c60b689ef91c40aeb93ede2467ea3bb5"
          }
        },
        "b7e63c909ce8403499ce4031be39cf96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_895ec9a4e5c0418881efcc1c45bea5f9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2.19k/? [00:00&lt;00:00, 2.38kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_26ee3d48d32f4f2fb1025ae9382d0854"
          }
        },
        "ddaad7ef3fd94f61bb9db437870987d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c60b689ef91c40aeb93ede2467ea3bb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "895ec9a4e5c0418881efcc1c45bea5f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "26ee3d48d32f4f2fb1025ae9382d0854": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "28e5e19bfb484be6adb61b929bfce731": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_452120c506764334ba48f529c66dd263",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_41c5abba5f6e4ad493b8b4cb39b98aef",
              "IPY_MODEL_9258c7d656d74e6582d41a3ed42f2d66"
            ]
          }
        },
        "452120c506764334ba48f529c66dd263": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "41c5abba5f6e4ad493b8b4cb39b98aef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_845b752f208f4250b1549a3de8134fa9",
            "_dom_classes": [],
            "description": "Downloading: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 8116577,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 8116577,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c65f1ebf3c944adf938188012ab40bde"
          }
        },
        "9258c7d656d74e6582d41a3ed42f2d66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c34d6f18051f455597ecb467c7163fa3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 30.3M/? [00:00&lt;00:00, 40.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0842c7f198f54a3ea30d41bf540ec6b5"
          }
        },
        "845b752f208f4250b1549a3de8134fa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c65f1ebf3c944adf938188012ab40bde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c34d6f18051f455597ecb467c7163fa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0842c7f198f54a3ea30d41bf540ec6b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3b6a5259bdc348e792a18fe1db767c68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ddcc69c02dd54f41b75e2a9f716a4845",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f3d52820256b497cbde17ca7cae166b6",
              "IPY_MODEL_46ca9dac532e411fbd846e593bc0b959"
            ]
          }
        },
        "ddcc69c02dd54f41b75e2a9f716a4845": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f3d52820256b497cbde17ca7cae166b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e9c74fdaf29f42a197af7cb1d0f4abd1",
            "_dom_classes": [],
            "description": "Downloading: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1054280,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1054280,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_015b3174469e4eca9cbac30a1d503efb"
          }
        },
        "46ca9dac532e411fbd846e593bc0b959": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_db3dcc72c37a417992e62b5bfa4a5b4f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4.85M/? [00:00&lt;00:00, 17.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8cfb9b3048344cffa1fc3712d63c6eb0"
          }
        },
        "e9c74fdaf29f42a197af7cb1d0f4abd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "015b3174469e4eca9cbac30a1d503efb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "db3dcc72c37a417992e62b5bfa4a5b4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8cfb9b3048344cffa1fc3712d63c6eb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "21fcba1f6af04c0a9d566966cc75b20a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_42d4351c5e794103ae4265bc0f5a8c67",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e617c8df23cc443689178dc078f3f58c",
              "IPY_MODEL_9c9f33da78f345cfb267fa0ae893c266"
            ]
          }
        },
        "42d4351c5e794103ae4265bc0f5a8c67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e617c8df23cc443689178dc078f3f58c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e0b4c9c0297d4785a91e8c6f3320da61",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e4c4239987d048aeaf90bd8f023d68ea"
          }
        },
        "9c9f33da78f345cfb267fa0ae893c266": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9d3af22d3128485b98a374a913a88b7b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 87599/0 [00:10&lt;00:00, 18618.04 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dcb444e18e0147e2ae29b87e83e9e08a"
          }
        },
        "e0b4c9c0297d4785a91e8c6f3320da61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e4c4239987d048aeaf90bd8f023d68ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9d3af22d3128485b98a374a913a88b7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dcb444e18e0147e2ae29b87e83e9e08a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ba47d0cdc7764c2ebf5fc0a99be0befb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0ed6f7466fa0409aaa8845bbd3b001eb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_dab9a0d7d9d04c7a9edff8d078cf0710",
              "IPY_MODEL_4fc85fddd0344a13927b1bf2931bb4ae"
            ]
          }
        },
        "0ed6f7466fa0409aaa8845bbd3b001eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dab9a0d7d9d04c7a9edff8d078cf0710": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_87f68bc8effb4e03a8a6cdaa2bd2ca6e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7c5545138aab4f1ca5bac55ca2d79ada"
          }
        },
        "4fc85fddd0344a13927b1bf2931bb4ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4ae2cc039456455fbd827ae068ca3ce9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 10570/0 [00:00&lt;00:00, 7916.92 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8c04cf0473124aedb535930cd9754cfa"
          }
        },
        "87f68bc8effb4e03a8a6cdaa2bd2ca6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7c5545138aab4f1ca5bac55ca2d79ada": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4ae2cc039456455fbd827ae068ca3ce9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8c04cf0473124aedb535930cd9754cfa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLvb1hyvDUT7"
      },
      "source": [
        "# NLP Assignment #03 - Language Model and Attention\n",
        "#### Natural Language Processing, Khatam University, Winter 1399\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhNp1J6gE29p"
      },
      "source": [
        "**Please pay attention to these notes:**\n",
        "<br><br>\n",
        "\n",
        "\n",
        "- **Assignment Due:** <b><font color='red'>1400.02.23</font></b> 23:59:00\n",
        "- If you need any additional information, please review the assignment page on the course website.\n",
        "- The items you need to answer are highlighted in <font color=\"SeaGreen\">**bold SeaGreen**</font> and the coding parts you need to implement are denoted by:\n",
        "```\n",
        "# ------------------\n",
        "# Put your implementation here     \n",
        "# ------------------\n",
        "```\n",
        "for a block of codes and\n",
        "```\n",
        "\"\"\" Implement this \"\"\"\n",
        "```\n",
        "for inline codes.\n",
        "\n",
        "\n",
        "\n",
        "- We always recommend co-operation and discussion in groups for assignments. However, **each student has to finish all the questions by him/herself**. If our matching system identifies any sort of copying, you'll be responsible for consequences.\n",
        "- Students who audit this course should submit their assignments like other students to be qualified for attending the rest of the sessions.\n",
        "- If you have any questions about this assignment, feel free to drop us a line. You may also post your questions on the course Microsoft Teams channel.\n",
        "- You must run this notebook on Google Colab platform, it depends on Google Colab VM for some of the depencecies.\n",
        "- You can double click on collapsed code cells to expand them.\n",
        "- <b><font color='red'>When you are ready to submit, please follow the instructions at the end of this notebook.</font></b>\n",
        "\n",
        "\n",
        "<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jKufIRlF9X7"
      },
      "source": [
        "#Introduction\n",
        "\n",
        "In this assignment, we will try to train a language model using both statistical and deep learning approaches. Then we will compose an attentive encoder-decoder neural network to train a model which translate English sentences to Persian sentences. After training our attentive translation model, we will check it's sanity by visualizing its attention weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTdnoz_rHaMB"
      },
      "source": [
        "# Build a tokenizer from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVlAwn7zNas5"
      },
      "source": [
        "Before we get to training our language models, we need to split input texts into smaller chunks (words or subwords) called tokens, which then are converted to ids through a look-up table. To this end, we use HuggingFace library and build our tokenization from scratch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQ5I8YND-0uH"
      },
      "source": [
        "from IPython.display import clear_output \n",
        "\n",
        "!pip install tokenizers datasets\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65sqdWSM-396"
      },
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.models import WordPiece\n",
        "\n",
        "from tokenizers import normalizers\n",
        "from tokenizers.normalizers import Lowercase, NFD, Strip, StripAccents \n",
        "\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.trainers import WordPieceTrainer\n",
        "\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.processors import TemplateProcessing\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Onb5yUshRhrb"
      },
      "source": [
        "We reserve four special tokens in the tokenizerâ€™s vocabulary:\n",
        "\n",
        "- [BOS]: the token at the start of each sentence\n",
        "\n",
        "- [EOS]: the token at the end of each sentence \n",
        "\n",
        "- [UNK]: indicate an out of vocabulary token\n",
        "\n",
        "- [PAD]: indicate token which shall be ignored by the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRiIEolk-p0R"
      },
      "source": [
        "def build_tokenizer(corpus,\n",
        "                    algorithm='WordLevel',\n",
        "                    vocab_size=7000,\n",
        "                    max_length=None,\n",
        "                    post_processor=True):\n",
        "\n",
        "  if algorithm == 'BPE':\n",
        "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "  elif algorithm == 'WordPiece':\n",
        "    tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
        "  else:\n",
        "    tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
        "\n",
        "\n",
        "  tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), Strip(), StripAccents()])\n",
        "  tokenizer.pre_tokenizer = Whitespace() \n",
        "\n",
        "  if algorithm == 'BPE':\n",
        "    trainer = BpeTrainer(special_tokens=['[PAD]', '[UNK]', '[BOS]', '[EOS]'], vocab_size=vocab_size)\n",
        "  elif algorithm == 'WordPiece':\n",
        "    trainer = WordPieceTrainer(special_tokens=['[PAD]', '[UNK]', '[BOS]', '[EOS]'], vocab_size=vocab_size)\n",
        "  else:\n",
        "    trainer = WordLevelTrainer(special_tokens=['[PAD]', '[UNK]', '[BOS]', '[EOS]'], vocab_size=vocab_size)\n",
        "\n",
        "  \n",
        "  if max_length:\n",
        "    tokenizer.enable_padding(length=max_length, pad_token=\"[PAD]\")\n",
        "    tokenizer.enable_truncation(max_length=max_length)\n",
        "\n",
        "  tokenizer.train_from_iterator(corpus, trainer)\n",
        "\n",
        "  if post_processor:\n",
        "    tokenizer.post_processor = TemplateProcessing(\n",
        "      single=\"[BOS] $A [EOS]\",\n",
        "      special_tokens=[\n",
        "          (\"[BOS]\", tokenizer.token_to_id(\"[BOS]\")),\n",
        "          (\"[EOS]\", tokenizer.token_to_id(\"[EOS]\")),\n",
        "      ]\n",
        "    )\n",
        "\n",
        "  return tokenizer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY4wDjC3E29Q"
      },
      "source": [
        " Letâ€™s train a tokenizer on contexts in squad dataset (87599 of plain texts):\n",
        "\n",
        "Feel free to choose tokenization algorithm between WordLevel, BPE and WordPiece, but be consistent in using your opted algorithm through the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2qwO96v48KM"
      },
      "source": [
        "**Question**: \n",
        "\n",
        "<b><font color=\"SeaGreen\">Study about WordLevel, WordPiece, and Byte-Pair Encoding (BPE) algorithms, briefly explain how they work, and compare them.</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wm4WTDMn8DRP"
      },
      "source": [
        "<b><font color=\"SeaGreen\">Write your answers here...</font></b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269,
          "referenced_widgets": [
            "a13ec92f6fd04006b0c35f84f16dfe7e",
            "59311c7c313b4eda91ef6bbb01fd39d3",
            "fd4e945ebf3941ad831febb248b6d44f",
            "4dd2f5ea8fd14cf0b6a336d3a2fdcdb0",
            "c83a6bfa67f443d08addf471b2651ead",
            "3635577b6ab94b838efd7d4cab4a27ec",
            "39d6d7882798403ba8a39e3170de46ef",
            "da4d7885012145969732add4e27a6efa",
            "61fd1687e0674e099b33354bdead4317",
            "8e2b3dda0e114fe9b7127ca2ceeb35a3",
            "c587a678e48541cf8eb5b9d412c0fda4",
            "b7e63c909ce8403499ce4031be39cf96",
            "ddaad7ef3fd94f61bb9db437870987d8",
            "c60b689ef91c40aeb93ede2467ea3bb5",
            "895ec9a4e5c0418881efcc1c45bea5f9",
            "26ee3d48d32f4f2fb1025ae9382d0854",
            "28e5e19bfb484be6adb61b929bfce731",
            "452120c506764334ba48f529c66dd263",
            "41c5abba5f6e4ad493b8b4cb39b98aef",
            "9258c7d656d74e6582d41a3ed42f2d66",
            "845b752f208f4250b1549a3de8134fa9",
            "c65f1ebf3c944adf938188012ab40bde",
            "c34d6f18051f455597ecb467c7163fa3",
            "0842c7f198f54a3ea30d41bf540ec6b5",
            "3b6a5259bdc348e792a18fe1db767c68",
            "ddcc69c02dd54f41b75e2a9f716a4845",
            "f3d52820256b497cbde17ca7cae166b6",
            "46ca9dac532e411fbd846e593bc0b959",
            "e9c74fdaf29f42a197af7cb1d0f4abd1",
            "015b3174469e4eca9cbac30a1d503efb",
            "db3dcc72c37a417992e62b5bfa4a5b4f",
            "8cfb9b3048344cffa1fc3712d63c6eb0",
            "21fcba1f6af04c0a9d566966cc75b20a",
            "42d4351c5e794103ae4265bc0f5a8c67",
            "e617c8df23cc443689178dc078f3f58c",
            "9c9f33da78f345cfb267fa0ae893c266",
            "e0b4c9c0297d4785a91e8c6f3320da61",
            "e4c4239987d048aeaf90bd8f023d68ea",
            "9d3af22d3128485b98a374a913a88b7b",
            "dcb444e18e0147e2ae29b87e83e9e08a",
            "ba47d0cdc7764c2ebf5fc0a99be0befb",
            "0ed6f7466fa0409aaa8845bbd3b001eb",
            "dab9a0d7d9d04c7a9edff8d078cf0710",
            "4fc85fddd0344a13927b1bf2931bb4ae",
            "87f68bc8effb4e03a8a6cdaa2bd2ca6e",
            "7c5545138aab4f1ca5bac55ca2d79ada",
            "4ae2cc039456455fbd827ae068ca3ce9",
            "8c04cf0473124aedb535930cd9754cfa"
          ]
        },
        "id": "MnldZxfN7mCm",
        "outputId": "e858cd7c-493f-4782-8afb-04d9aa661fd4"
      },
      "source": [
        "VOCAB_SIZE = 10000\n",
        "\n",
        "squad_dataset = load_dataset('squad')\n",
        "raw_corpus = []\n",
        "for context in squad_dataset['train']['context']:\n",
        "  raw_corpus.append(context)\n",
        "\n",
        "tokenizer = build_tokenizer(raw_corpus, algorithm='WordLevel', vocab_size=VOCAB_SIZE)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a13ec92f6fd04006b0c35f84f16dfe7e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1877.0, style=ProgressStyle(descriptionâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "61fd1687e0674e099b33354bdead4317",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=955.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Downloading and preparing dataset squad/plain_text (download: 33.51 MiB, generated: 85.75 MiB, post-processed: Unknown size, total: 119.27 MiB) to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "28e5e19bfb484be6adb61b929bfce731",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=8116577.0, style=ProgressStyle(descriptâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b6a5259bdc348e792a18fe1db767c68",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1054280.0, style=ProgressStyle(descriptâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "21fcba1f6af04c0a9d566966cc75b20a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba47d0cdc7764c2ebf5fc0a99be0befb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\rDataset squad downloaded and prepared to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a. Subsequent calls will reuse this data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DC9iRIuGWX_"
      },
      "source": [
        "Now that we have trained a tokenizer, we can use it on any text we want with the encode() method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfH12af3LoIm"
      },
      "source": [
        "output = tokenizer.encode(\"Tehran Institute for Advanced Studies\", add_special_tokens=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIS-DL5jHF_W"
      },
      "source": [
        "The tokens attribute contains the segmentation of your text in tokens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjS_VrHGMPMl",
        "outputId": "0b7707c9-f399-41a7-e14a-a2dadbe46f50"
      },
      "source": [
        "print(output.tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['tehran', 'institute', 'for', 'advanced', 'studies']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJrHXU7aHDHR"
      },
      "source": [
        "Similarly, the ids attribute will contain the index of each of those tokens in the tokenizerâ€™s vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lGdJJktMRw0",
        "outputId": "f2c7d836-ab22-4f82-b983-4651350a760f"
      },
      "source": [
        "print(output.ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 990, 20, 1426, 682]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkX0C_R6JKuu"
      },
      "source": [
        "The _decoder_ convert the IDs back to tokens (using the tokenizerâ€™s vocabulary):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "yF9vo1jjIwm0",
        "outputId": "61efbbec-d659-4247-9796-86b694e68e90"
      },
      "source": [
        "tokenizer.decode(output.ids, skip_special_tokens=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[UNK] institute for advanced studies'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAxpG_gQM3Lg",
        "outputId": "ad81c456-9c18-438e-b4bb-92c5baac7cbd"
      },
      "source": [
        "tokenizer.get_vocab_size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpHn0HrHH9NO"
      },
      "source": [
        "# Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkRN_mGHfZI1"
      },
      "source": [
        "Here we gather plain texts in validation squad dataset and split them into sentences. You are allowed to use only this corpus (stored in lm_corpus list variable) to train your language models in this section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3Xm10qOeWU7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d51fa875-e536-4fbf-bf3a-d545ab8bca97"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "lm_corpus = []\n",
        "for context in squad_dataset['validation']['context']:\n",
        "  sentences = sent_tokenize(context)\n",
        "  sentences = [sent for sent in sentences]\n",
        "  lm_corpus.extend(sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEwcrjZqO8wR"
      },
      "source": [
        "## Statistics Approach:\n",
        "> ## Trigram Language Model\n",
        "\n",
        "Here, we want to build a trigram language model in which each word only depends on 2 previous words.\n",
        "$$ P(w_1, \\dots, w_n) = \\prod_t P(w_t \\mid w_{t - 2}, w_{t - 1}). $$\n",
        "\n",
        "As you have seen before, two sparsity problem could be occure with such language models when:\n",
        "\n",
        "- $c(w_{t-2}, w_{t-1}, w_t)$ = 0\n",
        "\n",
        "- $c(w_{t-2}, w_{t-1})$ = 0\n",
        "\n",
        "\n",
        "To address this issues, leverage both __Add-one smoothing__ and __Backoff__ techniques in your implementation.\n",
        "\n",
        "Trigram version of Backofff is:\n",
        "$\\hat{P}(w_t \\mid w_{t-2} w_{t-1}) = \\begin{cases} P(w_t \\mid w_{t-2} w_{t-1}), & if \\; c(w_{t-2} w_{t-1} w_t) > 0 \n",
        "\\\\ \\alpha P(w_t \\mid w_{t-1}), & if \\; c(w_{t-2} w_{t-1} w_t) = 0 \\; and \\; c(w_{t-1} w_t) > 0\n",
        "\\\\ \\beta P(w_t), \\; otherwise \\end{cases}$\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaEcA3s1DhFH"
      },
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "class TrigramLanguageModel:    \n",
        "  def __init__(self, corpus, alpha, beta):\n",
        "    \"\"\" \n",
        "    A simple count-based language model: \n",
        "    compute P(w_t | prefix) given ngram counts\n",
        "    # computes probability of next token given two previous tokens\n",
        "    :param data: list of sentences\n",
        "    :param alpha\n",
        "    :param beta\n",
        "    \"\"\"\n",
        "    self.alpha = alpha\n",
        "    self.beta = beta\n",
        "\n",
        "    self.unigram_probs = defaultdict(Counter)\n",
        "    # probs[word1] = P(word1)\n",
        "\n",
        "    self.bigram_probs = defaultdict(Counter)\n",
        "    # probs[(word1)][word2] = P(word2 | word1)\n",
        "\n",
        "    self.trigram_probs = defaultdict(Counter)\n",
        "    # probs[(word1, word2)][word3] = P(word3 | word1, word2)\n",
        "\n",
        "    # ------------------\n",
        "    # Put your implementation here   \n",
        "    # ------------------\n",
        "            \n",
        "  def get_possible_next_tokens(self, prefix):\n",
        "    \"\"\"\n",
        "    :param prefix: a list of tokens corresponding to  a string\n",
        "    :returns: a dictionary {token : it's Laplacian probability} for all tokens with positive probabilities\n",
        "    \"\"\"\n",
        "\n",
        "    prefix = prefix[max(0, len(prefix) - 3):]\n",
        "    # ------------------\n",
        "    # Put your implementation here   \n",
        "    # ------------------\n",
        "    return probs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8lJEHiqMHH0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04fe83cf-70ce-47e4-abf2-d026c2db0ae7"
      },
      "source": [
        "dummy_lm = TrigramLanguageModel(corpus=lm_corpus, alpha=1, beta=1)\n",
        "\n",
        "p_initial = dummy_lm.get_possible_next_tokens(['the', 'united']) \n",
        "print(\"P(states | the, united): \", p_initial['states']) \n",
        "print(\"P(kingdom | the, united): \", p_initial['kingdom']) \n",
        "\n",
        "p_initial = dummy_lm.get_possible_next_tokens(['university', 'of']) \n",
        "print(\"\\nlist of possible tokens which could be appear after (there, is): \")\n",
        "for token, prob in p_initial.items():\n",
        "  print(token, \", \", prob)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "P(states | the, united):  0.025918730106237856\n",
            "P(kingdom | the, united):  0.006820718449009963\n",
            "\n",
            "list of possible tokens which could be appear after (there, is): \n",
            "warsaw ,  0.0011002962336013541\n",
            "technology ,  0.0004655099449851883\n",
            "music ,  0.0002539145154464664\n",
            "life ,  0.0002539145154464664\n",
            "erfurt ,  0.0002539145154464664\n",
            "wittenberg ,  0.0006771053745239103\n",
            "oxford ,  0.0004655099449851883\n",
            "nottingham ,  0.00021159542953872197\n",
            "california ,  0.0004655099449851883\n",
            "redlands ,  0.0002539145154464664\n",
            "san ,  0.0002539145154464664\n",
            "southern ,  0.0002539145154464664\n",
            "melbourne ,  0.0002539145154464664\n",
            "paris ,  0.0004655099449851883\n",
            "florida ,  0.0004655099449851883\n",
            "michigan ,  0.00016927634363097757\n",
            "newcastle ,  0.0002539145154464664\n",
            "the ,  0.0002539145154464664\n",
            "northumbria ,  0.0002539145154464664\n",
            "dundee ,  0.0002539145154464664\n",
            "abertay ,  0.0002539145154464664\n",
            "cambridge ,  0.0003808717731696995\n",
            "north ,  0.0002539145154464664\n",
            "washington ,  0.00042319085907744394\n",
            "chicago ,  0.010664409648751587\n",
            "edinburgh ,  0.0002539145154464664\n",
            "aberdeen ,  0.0002539145154464664\n",
            "denver ,  0.00016927634363097757\n",
            "evansville ,  0.00016927634363097757\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_75m5FrqW7U"
      },
      "source": [
        "### Text Generation with Trigram LM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9F4vrzXvvJN6"
      },
      "source": [
        "The process of generating sentences is straightforward. You need to maintain a prefix string (a list of tokens) and iteratively add next token to that by sampling with probabilities.\n",
        "\n",
        "The most convenient approach is to choose the most likely token in each iteration, i.e., sort possible tokens based on their probabilities and take the first one. This is greedy generation.\n",
        "\n",
        "However, as you have seen in the lecture, greedy approach has no way to undo decisions and might to generate sentences full o repetitive words. Beam search is an good alternative. ([read more](https://huggingface.co/blog/how-to-generate))\n",
        "\n",
        "Hence, we expect you implement both greedy and Beam search approach and compare them in sentence generation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6cSLo0bXesp"
      },
      "source": [
        "def statistical_greedy_generate(lm, prefix='[BOS]', max_gen=20):\n",
        "  \"\"\"\n",
        "    :param prefix: a string of previous existing tokens\n",
        "    :param max_gen: maximum number of tokens to generate\n",
        "    :returns: most likely sentence\n",
        "  \"\"\"\n",
        "  num_generated_tokens = 0\n",
        "  \n",
        "  while True:\n",
        "    # ------------------\n",
        "    # Put your implementation here   \n",
        "    # ------------------\n",
        "\n",
        "    # Break loop if generated token is [EOS] or the number of generated tokens is equal to max_gen \n",
        "\n",
        "  return prefix\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39Z4YNx2ZdDA",
        "outputId": "81993c5c-efe4-457f-ea32-4e22ccd0cf2b"
      },
      "source": [
        "print(statistical_greedy_generate(dummy_lm, prefix='[BOS] this story is about', max_gen=20))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[BOS] this story is about the first time , and the first time , and the first time , and the first time , and the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsQ0rPMkH8y-"
      },
      "source": [
        "def statistical_beamsearch_generate(lm, prefix='[BOS]', max_gen=20, beam_size=4, top_k=50):\n",
        "  \"\"\"\n",
        "    :param prefix: a string of previous existing tokens\n",
        "    :param max_gen: maximum number of tokens to generate\n",
        "    :param beam_size: Number of beams for beam search. 1 means no beam search.\n",
        "    :param top_k: The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
        "    :returns: most likely sentence\n",
        "  \"\"\"\n",
        "  \n",
        "  # ------------------\n",
        "  # Put your implementation here   \n",
        "  # ------------------\n",
        "  \n",
        "  return #<most likely sentence>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_kuygzrPBbp"
      },
      "source": [
        "## Neural Approach: \n",
        "> ## RNN Language Model\n",
        "\n",
        "We have checked out sttistical approaches to train languae models in which we must select next possible token only based on a limited number of previous tokens like 3-gram. Here we want to see what Recurrent Neural \n",
        "Network has to offer. Such models processes one token at a time, left to right, and maintains a hidden state vector between them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34EO0ZH1tqa-"
      },
      "source": [
        "### Data Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MINnmY5FC2tI"
      },
      "source": [
        "To prepare the dataset for our neural model, we first concatenate all sentences in our dataset to form one single giant document. Then, we divide this document to fixed equal-length smaller sequences. \n",
        "In this setup, labels would be easily extracted by after token of each sequences in doucument. To have a better training, we avoid to model predict special tokens like out-of-vocabulary tokens in our document.\n",
        "Finally, the training batches are constructed by putting several sequences together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfvnB2-Mg7W6"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tf.config.optimizer.set_jit(True)\n",
        "\n",
        "def prepare_lm_dataset(data, window_size):\n",
        "\n",
        "  flat_ids = []\n",
        "  for sent in data:\n",
        "    flat_ids.extend(tokenizer.encode(sent).ids)\n",
        "\n",
        "  token_ids = []\n",
        "  label_ids = []\n",
        "\n",
        "  for i in range(len(flat_ids) - window_size):\n",
        "    if (flat_ids[window_size] == tokenizer.token_to_id('[UNK]') or\n",
        "        flat_ids[window_size] == tokenizer.token_to_id('[BOS]') or\n",
        "        flat_ids[window_size] == tokenizer.token_to_id('[EOS]') or\n",
        "        flat_ids[window_size] == tokenizer.token_to_id('[PAD]')\n",
        "    ):\n",
        "      continue\n",
        "    token_ids.append(flat_ids[i: i+window_size])\n",
        "    label_ids.append(flat_ids[i+window_size])\n",
        "\n",
        "\n",
        "  def gen():\n",
        "    for tokens, label in zip(token_ids, label_ids):\n",
        "      yield (tokens, label)\n",
        "\n",
        "  return tf.data.Dataset.from_generator(gen,\n",
        "                                       (tf.int64, tf.int64),\n",
        "                                       (tf.TensorShape([None]), tf.TensorShape([]))), len(token_ids)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyoLuxW6PGUb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2df30383-ab1d-489f-843d-54f6731e17aa"
      },
      "source": [
        "# Choose your desirable hyper params:\n",
        "batch_size = 64\n",
        "window_size = 32\n",
        "\n",
        "lm_train_dataset, num_train_examples = prepare_lm_dataset(lm_corpus, window_size=window_size)\n",
        "lm_train_dataset = lm_train_dataset.shuffle(buffer_size=100).batch(batch_size).cache().repeat()\n",
        "train_steps = int(np.ceil(num_train_examples / batch_size))\n",
        "\n",
        "print(num_train_examples)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1657077\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vZKQZ4mj0yk"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Je8NdcAr3Agc"
      },
      "source": [
        "class RNN_LM(tf.keras.models.Model):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # feel free to choose your desirable architecture but you must use at least an Embedding and Recurrent layer like Simple RNN or LSTM\n",
        "    # ------------------\n",
        "    # Put your implementation here   \n",
        "    # ------------------\n",
        "    \n",
        "  def call(self, inputs, training=False):\n",
        "    \n",
        "    # ------------------\n",
        "    # Put your implementation here   \n",
        "    # ------------------\n",
        "    return probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pjhkvvh3LHs"
      },
      "source": [
        "model = RNN_LM()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "model.compile(optimizer=\"Adam\", loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKpZqHtb0V9Z"
      },
      "source": [
        "history = model.fit(lm_train_dataset,\n",
        "                    epochs=15,\n",
        "                    steps_per_epoch=train_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1fvXS4BObwC"
      },
      "source": [
        "### Text Generation with Neural LM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUj55p98DxU6"
      },
      "source": [
        "def prepare_single_input(prefix, tokenizer, window_size):\n",
        "  input = tokenizer.encode(prefix).ids\n",
        "  \n",
        "  if len(input) > window_size:# truncate\n",
        "    input = input[len(input) - window_size:]\n",
        "  else: # padding\n",
        "    input = [tokenizer.token_to_id('[PAD]')] * (window_size - len(input)) + input\n",
        "\n",
        "  input = tf.convert_to_tensor(input)\n",
        "  input = tf.expand_dims(input, axis=0)\n",
        "\n",
        "  return input\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1Ui5LVZOg8S"
      },
      "source": [
        "def neural_greedy_generate(model, tokenizer, prefix='', max_gen=20, window_size=16):\n",
        "    \n",
        "  num_generated_tokens = 0\n",
        "  \n",
        "  while True:\n",
        "    input = prepare_single_input(prefix, tokenizer, window_size)\n",
        "    # ------------------\n",
        "    # Put your implementation here   \n",
        "    # ------------------\n",
        "    # Break loop if generated token is [EOS] or the number of generated tokens is equal to max_gen \n",
        "  return prefix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDOW9NX7RWCz"
      },
      "source": [
        "neural_greedy_generate(model, tokenizer, prefix=\"This story is about\", max_gen=20, window_size=16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZKldS2IbOdl"
      },
      "source": [
        "def neural_beamsearch_generate(model, tokenizer, prefix='', max_gen=20, window_size=16, beam_size=4, top_k=50):\n",
        "      \n",
        "  # ------------------\n",
        "  # Put your implementation here   \n",
        "  # ------------------\n",
        "  \n",
        "  return #<most likely sentence>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxaZkmeCOuSO"
      },
      "source": [
        "# Neural Machine Translation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWunAkI2Oxwj",
        "cellView": "form"
      },
      "source": [
        "# @title Download Dataset\n",
        "!wget https://github.com/omidkashefi/Mizan/raw/master/mizan.zip\n",
        "!unzip mizan.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gbK6A6vS2BW"
      },
      "source": [
        "### Load Dataset & Build Tokenizer\n",
        "\n",
        "\n",
        "In this section we load the MIZAN ([read more](https://arxiv.org/pdf/1801.02107v2.pdf)) Persian-English parallel corpus which features nearly 1M sentence pairs collected from masterpieces of literature.\n",
        "\n",
        "This time we need to build two separate tokenizers for each language and we use each language sentences as its corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZPToxHXJUxO"
      },
      "source": [
        "MAX_LENGTH = 10\n",
        "VOCAB_SIZE = 10000\n",
        "\n",
        "def load_mizan(url_en, url_fa):\n",
        "  english = []\n",
        "  persian = []\n",
        "  with open(url_en) as f_a, open(url_fa) as f_b:\n",
        "    for i in range(1000000):\n",
        "      line_a = f_a.readline()\n",
        "      line_b = f_b.readline()\n",
        "\n",
        "      if len(line_a.split()) > MAX_LENGTH or len(line_b.split()) > MAX_LENGTH - 1:\n",
        "        continue\n",
        "      english.append(line_a)\n",
        "      persian.append(line_b)\n",
        "  print(len(english))\n",
        "  return english, persian\n",
        "\n",
        "en, fa = load_mizan('mizan/mizan_en.txt', 'mizan/mizan_fa.txt')\n",
        "\n",
        "### We don't need the [BOS] & [EOS] tokens for the encoder\n",
        "en_tokenizer = build_tokenizer(en, \n",
        "                               max_length=MAX_LENGTH,\n",
        "                               vocab_size=VOCAB_SIZE,\n",
        "                               post_processor=False)\n",
        "\n",
        "fa_tokenizer = build_tokenizer(fa, \n",
        "                               max_length=MAX_LENGTH,\n",
        "                               vocab_size=VOCAB_SIZE,\n",
        "                               post_processor=True)\n",
        "\n",
        "BOS_ID = en_tokenizer.token_to_id(\"[BOS]\")\n",
        "EOS_ID = en_tokenizer.token_to_id(\"[EOS]\")\n",
        "PAD_ID = en_tokenizer.token_to_id(\"[PAD]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZvASryJTsKS"
      },
      "source": [
        "## Overview\n",
        "Sequence-to-sequence (seq2seq) models\n",
        "([Sutskever et al., 2014](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf),\n",
        "[Cho et al., 2014](http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf)) have\n",
        "enjoyed great success in a variety of tasks such as machine translation, speech\n",
        "recognition, and text summarization. We focus on the task of Neural Machine Translation (NMT)\n",
        "which was the very first testbed for seq2seq models with\n",
        "wild success.\n",
        "<br>\n",
        "<p align=\"center\">\n",
        "<img width=\"50%\" src=\"https://lena-voita.github.io/resources/lectures/seq2seq/general/enc_dec-min.png\" />\n",
        "<br>\n",
        "Figure 1. <b>Encoder-decoder architecture</b> â€“ example of a general approach for\n",
        "NMT. An encoder converts a source sentence into a \"meaning\" vector which is\n",
        "passed through a <i>decoder</i> to produce a translation.\n",
        "(Image borrowed from <a href=\"https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html\">Seq2Seq by Voita </a>)</p>\n",
        "\n",
        "\n",
        "Specifically, an NMT system first reads the source sentence using an *encoder*\n",
        "to build\n",
        "a \"thought\" vector,\n",
        "a sequence of numbers that represents the sentence meaning; a *decoder*, then,\n",
        "processes the sentence vector to emit a translation, as illustrated in\n",
        "Figure 1.\n",
        "\n",
        "To learn more about Seq2Seq check out [this link](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html).\n",
        "\n",
        "<sub>*Description borrowed from [Tensorflow's NMT Tutorial](https://github.com/tensorflow/nmt)*<sub>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SjUKbOITab-"
      },
      "source": [
        "### Data generator\n",
        "\n",
        "To train such models, we need to structurize the data. Each example has a source and a target sentence which each token of the target is predicted step by step.\n",
        "\n",
        "So each in each training data, we feed all the source sentence's tokens, the previous history of the target sentence (revealed or predicted tokens) to the model and we the next token id would be the prediction label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQZr8uA8JnGv"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tf.config.optimizer.set_jit(True)\n",
        "\n",
        "def prepare_train_mt_dataset(data_a, data_b, tokenizer_a, tokenizer_b, no_examples):\n",
        "\n",
        "  encodeds_a = tokenizer_a.encode_batch(data_a)\n",
        "  encodeds_b = tokenizer_b.encode_batch(data_b)\n",
        "\n",
        "  def gen():\n",
        "      for i in range(no_examples):\n",
        "\n",
        "        out_a = encodeds_a[i]\n",
        "        out_b = encodeds_b[i]\n",
        "\n",
        "        first_dict = {\n",
        "            \"ids\": tf.constant(out_a.ids),\n",
        "            \"mask\": tf.constant(out_a.attention_mask)\n",
        "        }\n",
        "        \n",
        "        max_len = len(out_b.ids)\n",
        "        for j in range(1, max_len):\n",
        "          extended_mask_len = max_len - j\n",
        "          label = out_b.ids[j]\n",
        "          if out_b.ids[j-1] == 3:\n",
        "            break\n",
        "          second_dict = {\n",
        "            \"ids\": tf.constant(out_b.ids[:j] + [0] * extended_mask_len),\n",
        "            \"mask\": tf.constant(out_b.attention_mask[:j] + [0] * extended_mask_len)\n",
        "          }\n",
        "          yield (\n",
        "              (first_dict,\n",
        "              second_dict),\n",
        "              label\n",
        "          )\n",
        "\n",
        "  return tf.data.Dataset.from_generator(gen,\n",
        "                                       (({\"ids\": tf.int32,\n",
        "                                         \"mask\": tf.int32},\n",
        "                                         {\"ids\": tf.int32,\n",
        "                                         \"mask\": tf.int32}), \n",
        "                                        tf.int32),\n",
        "                                       (({\"ids\": tf.TensorShape([None]),\n",
        "                                         \"mask\": tf.TensorShape([None])},\n",
        "                                         {\"ids\": tf.TensorShape([None]),\n",
        "                                         \"mask\": tf.TensorShape([None])}),\n",
        "                                        tf.TensorShape([])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-uef1o-Uk0W"
      },
      "source": [
        "### Load MT dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwtv4xnELDYS"
      },
      "source": [
        "mt_dataset = prepare_train_mt_dataset(en,\n",
        "                                      fa,\n",
        "                                      en_tokenizer,\n",
        "                                      fa_tokenizer,\n",
        "                                      no_examples=len(en))\n",
        "\n",
        "BATCH_SIZE = 1024\n",
        "training_steps = int(np.ceil(3174400 // BATCH_SIZE))\n",
        "train_dataset = mt_dataset.batch(BATCH_SIZE).take(training_steps).cache().repeat()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQrBWYsBU2kg"
      },
      "source": [
        "### RNN Encoder-Decoder Model\n",
        "\n",
        "In this section you should implement a RNN-based encoder-decoder model for the translation task. The encoder is an RNN which encodes the sentence and passes its final states as initial states for the decoder (also an RNN).\n",
        "\n",
        "(Hint: The minimal ingredients are: two RNN modules(LSTM or GRU), two embedding layers and a classifier. And don't forget about using the masks!)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVSrgpojLRnX"
      },
      "source": [
        "class RNN_MT(tf.keras.models.Model):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # ------------------\n",
        "    # Put your implementation here   \n",
        "    # ------------------\n",
        "\n",
        "  def encode(self, inputs, training=False):\n",
        "    # ------------------\n",
        "    # Put your implementation here   \n",
        "    # ------------------\n",
        "    return lstm_outputs, lstm_h, lstm_c\n",
        "\n",
        "  def decode(self, inputs, initial_state, training=False):\n",
        "    # ------------------\n",
        "    # Put your implementation here   \n",
        "    # ------------------\n",
        "    return probs\n",
        "\n",
        "  def translate(self, src_inputs):\n",
        "    decoder_ids = [BOS_ID] \n",
        "    decoder_mask = [1] \n",
        "    for i in range(1, MAX_LENGTH):\n",
        "      ### Complete Calling the model to decode next token\n",
        "      probs = self.call((src_inputs, {\n",
        "          \"ids\": np.array([\"\"\" Implement this \"\"\"]),\n",
        "          \"mask\": np.array([\"\"\" Implement this \"\"\"])\n",
        "      }))\n",
        "\n",
        "      ### Select top token by highest prob.\n",
        "      top_id = \"\"\" Implement this \"\"\"\n",
        "\n",
        "      ### Append to the list of previous decoded tokens\n",
        "      # ------------------\n",
        "      # Put your implementation here   \n",
        "      # ------------------\n",
        "\n",
        "      ### Check whether predicted token is [EOS] => break the loop\n",
        "      # ------------------\n",
        "      # Put your implementation here   \n",
        "      # ------------------\n",
        "\n",
        "      ### Extend the attention mask\n",
        "      decoder_mask.append(1)\n",
        "    return decoder_ids\n",
        "\n",
        "  def call(self, inputs, training=False):\n",
        "    _, h, c = self.encode(inputs[0], training=training)\n",
        "    probs = self.decode(inputs[1], [h, c], training=training)\n",
        "    return probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEMzyZ2DX8CR"
      },
      "source": [
        "#### Compile & Train our RNN_MT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnRUhndYLznT"
      },
      "source": [
        "model = RNN_MT()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "model.fit(train_dataset, steps_per_epoch=training_steps, epochs=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnKSUObiYL6Y"
      },
      "source": [
        "#### Test the RNN\n",
        "\n",
        "This function should encode an english sentence using the `en_tokenizer`, build the input structure and then using the returned decoded ids from the model, it should decode the ids to a persian sentence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dHcwwOgWABi"
      },
      "source": [
        "def translate_to_fa(en_sentence, model):\n",
        "  encoded_en = ### Implement this\n",
        "  input_dict = {\n",
        "    \"ids\": np.array(\"\"\" Implement this \"\"\"]),\n",
        "    \"mask\": np.array(\"\"\" Implement this \"\"\"])\n",
        "  }\n",
        "  decoded_ids = ### Implement this\n",
        "  return fa_tokenizer.decode(decoded_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq0mcA1nYRn4"
      },
      "source": [
        "#### Evaluate with BLEU\n",
        "Now to figure out how accurately do the RNN_MT translates the source sentences we compute **BLEU** metric which is a commonly used metric in machine translation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiCW_XJFVeI_"
      },
      "source": [
        "def compute_BLEU(source, target):\n",
        "  # ------------------\n",
        "  # Put your implementation here   \n",
        "  # ------------------\n",
        "  return bleu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2aO1w3hZHFE"
      },
      "source": [
        "Let's see how our model performs in these examples below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIm45HB9Vu9h"
      },
      "source": [
        "sample_sentences_ids = [6645,4527,17891,14762,461913]\n",
        "\n",
        "for id in sample_sentences_ids:\n",
        "  translated_text = translate_to_fa(en[id], model)\n",
        "  print(\"En:\", en[id])\n",
        "  print(\"Target Fa:\", fa[id])\n",
        "  print(\"Translated Fa:\", translated_text)\n",
        "  print(\"BLEU:\", compute_BLEU(fa[id], translated_text), end=\"\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxKMplqAZYgA"
      },
      "source": [
        "### RNN Encoder-Decoder with Attention Model\n",
        "\n",
        "As you have seen in NLP lectures slides, attention mechanisms show significant improvements over NMT tasks. \n",
        "\n",
        "So, lets reimplement our RNN_MT model by adding an attention layer. You can use the same model that you have already implemented and just simply add the required layers or weights for the attention module.\n",
        "\n",
        "(Hint: This is commonly-used model architecture:\n",
        "<br>\n",
        "<p align=\"center\">\n",
        "<img width=\"50%\" src=\"https://lena-voita.github.io/resources/lectures/seq2seq/attention/luong_model-min.png\" /></p>\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a_-fE4RZgYH"
      },
      "source": [
        "class RNNwAttention_MT(tf.keras.models.Model):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # ------------------\n",
        "    # Put your implementation here   \n",
        "    # ------------------\n",
        "\n",
        "  def encode(self, inputs, training=False):\n",
        "    # ------------------\n",
        "    # Put your implementation here   \n",
        "    # ------------------\n",
        "    return lstm_outputs, lstm_h, lstm_c\n",
        "\n",
        "  def decode(self, inputs, initial_state, encoder_outputs, training=False):\n",
        "    # ------------------\n",
        "    # Put your implementation here   \n",
        "    # ------------------\n",
        "    return probs, att_scores\n",
        "\n",
        "  def translate(self, src_inputs, return_attentions=False):\n",
        "    decoder_ids = [BOS_ID] \n",
        "    decoder_mask = [1] \n",
        "    attention_map = []\n",
        "    for i in range(1, MAX_LENGTH):\n",
        "      ### Call model to decode next token (and retrieve the attention scores if return_attentions is True)\n",
        "      # ------------------\n",
        "      # Put your implementation here   \n",
        "      # ------------------\n",
        "\n",
        "      ### Select top token by highest prob.\n",
        "      top_id = \"\"\" Implement this \"\"\"\n",
        "\n",
        "      ### Append to the list of previous decoded tokens\n",
        "      # ------------------\n",
        "      # Put your implementation here   \n",
        "      # ------------------\n",
        "      \n",
        "      ### Append the attention scores to the attention map\n",
        "      if return_attentions:\n",
        "        attention_map.append(att_scores)\n",
        "\n",
        "      ### Check whether predicted token is [EOS] => break the loop\n",
        "      # ------------------\n",
        "      # Put your implementation here   \n",
        "      # ------------------\n",
        "\n",
        "      ### Extend the attention mask\n",
        "      decoder_mask.append(1)\n",
        "      \n",
        "    if return_attentions:\n",
        "      return decoder_ids, np.array(attention_map))\n",
        "    return decoder_ids\n",
        "\n",
        "  def call(self, inputs, training=False, return_attentions=False):\n",
        "    enc_outputs, h, c = self.encode(inputs[0], training=training)\n",
        "    dec_outputs, att_scores = self.decode(inputs[1], [h, c], enc_outputs, training=training)\n",
        "    if return_attentions:\n",
        "      return dec_outputs, att_scores\n",
        "    return dec_outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM5GXCR2cCAB"
      },
      "source": [
        "#### Compile & Train our RNNwAttention_MT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfeFKfrjcBe8"
      },
      "source": [
        "model_wAtt = RNN_MT()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "model_wAtt.compile(optimizer='adam', loss=loss)\n",
        "model_wAtt.fit(train_dataset, steps_per_epoch=training_steps, epochs=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyUPQfJjcMRd"
      },
      "source": [
        "#### Evaluate with BLEU\n",
        "\n",
        "Now lets see how the attention mechanism strengthens the performance!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pf0WpFgscf8G"
      },
      "source": [
        "sample_sentences_ids = [6645,4527,17891,14762,461913]\n",
        "\n",
        "for id in sample_sentences_ids:\n",
        "  translated_text = translate_to_fa(en[id], model_wAtt)\n",
        "  print(\"En:\", en[id])\n",
        "  print(\"Target Fa:\", fa[id])\n",
        "  print(\"Translated Fa:\", translated_text)\n",
        "  print(\"BLEU:\", compute_BLEU(fa[id], translated_text), end=\"\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whniyMFXc3k6"
      },
      "source": [
        "#### Visualize the attention maps\n",
        "\n",
        "To visualize how the attention mechanism works, one solution is to plot the attention scores. \n",
        "In this section plot the attention scores outputted by the model and see how in each sentence how each token used the other languages tokens to form itself.\n",
        "\n",
        "**Note** - Your final plot should look like this:\n",
        "\n",
        "![download.png](data:image/png;base64,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)\n",
        "\n",
        "To add persian text to the plot you'll need the following line to repair the encoding issues:\n",
        "```\n",
        "get_display( arabic_reshaper.reshape( your text ))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FR9F5pB1qafx"
      },
      "source": [
        "!pip install arabic_reshaper python-bidi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hm3XSvgRd2n3"
      },
      "source": [
        "import arabic_reshaper\n",
        "import matplotlib.pyplot as plt\n",
        "from seaborn import heatmap\n",
        "from bidi.algorithm import get_display\n",
        "\n",
        "def visualize_map(en_sentence):\n",
        "  encoded_en = ### Implement this\n",
        "  input_dict = {\n",
        "    \"ids\": np.array(\"\"\" Implement this \"\"\"]),\n",
        "    \"mask\": np.array(\"\"\" Implement this \"\"\"])\n",
        "  }\n",
        "  decoded_ids, attention_map = ### Implement this\n",
        "\n",
        "  ### plotting the attention map\n",
        "  # ------------------\n",
        "  # Put your implementation here   \n",
        "  # ------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vou7kMeneeOC"
      },
      "source": [
        "### Run without change\n",
        "sample_sentences_ids = [13205, 13208, 21428, 300223, 432110]\n",
        "\n",
        "for id in sample_sentences_ids:\n",
        "  visualize_map(en[id])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyngqyHEBLtg"
      },
      "source": [
        "# Submission\n",
        "\n",
        "Congratulations! You finished the assignment & you're ready to submit your work. Please follow the instructions:\n",
        "\n",
        "1. Check and review your answers. Make sure all of the cell outputs are what you want. \n",
        "2. Select File > Save.\n",
        "3. **Fill your information** & run the cell bellow.\n",
        "4. Run **Make Submission** cell, It may take several minutes and it may ask you for your credential.\n",
        "5. Run **Download Submission** cell to obtain your submission as a zip file.\n",
        "6. Grab the downloaded file (`nlp_asg03__xx__xx.zip`) and hand it over in microsoft teams."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMnuRBKzBTRm"
      },
      "source": [
        "## Fill your information (Run the cell)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v326AywIBJwl"
      },
      "source": [
        "#@title Enter your information & \"RUN the cell!!\" { run: \"auto\" }\n",
        "student_id = \"\" #@param {type:\"string\"}\n",
        "student_name = \"\" #@param {type:\"string\"}\n",
        "\n",
        "print(\"your student id:\", student_id)\n",
        "print(\"your name:\", student_name)\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "ASSIGNMENT_PATH = Path('asg03')\n",
        "ASSIGNMENT_PATH.mkdir(parents=True, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzaBlkhkBWMu"
      },
      "source": [
        "## Make Submission (Run the cell)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOFCk9ebBXlK"
      },
      "source": [
        "#@title Make submission\n",
        "! pip install -U --quiet PyDrive > /dev/null\n",
        "! pip install -U --quiet jdatetime > /dev/null\n",
        "\n",
        "# ! wget -q https://github.com/github/hub/releases/download/v2.10.0/hub-linux-amd64-2.10.0.tgz \n",
        "\n",
        "\n",
        "import os\n",
        "import time\n",
        "import yaml\n",
        "import json\n",
        "import jdatetime\n",
        "\n",
        "from google.colab import files\n",
        "from IPython.display import Javascript\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "asg_name = 'NLP_Assignment_3'\n",
        "script_save = '''\n",
        "require([\"base/js/namespace\"],function(Jupyter) {\n",
        "    Jupyter.notebook.save_checkpoint();\n",
        "});\n",
        "'''\n",
        "# repo_name = 'iust-deep-learning-assignments'\n",
        "submission_file_name = 'nlp_asg02__%s__%s.zip'%(student_id, student_name.lower().replace(' ',  '_'))\n",
        "\n",
        "sub_info = {\n",
        "    'student_id': student_id,\n",
        "    'student_name': student_name, \n",
        "    'dateime': str(jdatetime.date.today()),\n",
        "    'asg_name': asg_name\n",
        "}\n",
        "json.dump(sub_info, open('info.json', 'w'))\n",
        "\n",
        "Javascript(script_save)\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "file_id = drive.ListFile({'q':\"title='%s.ipynb'\"%asg_name}).GetList()[0]['id']\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('%s.ipynb'%asg_name) \n",
        "\n",
        "! jupyter nbconvert --to script \"$asg_name\".ipynb > /dev/null\n",
        "! jupyter nbconvert --to html \"$asg_name\".ipynb > /dev/null\n",
        "! zip \"$submission_file_name\" \"$asg_name\".ipynb \"$asg_name\".html \"$asg_name\".txt info.json > /dev/null\n",
        "\n",
        "print(\"##########################################\")\n",
        "print(\"Done! Submisson created, Please download using the bellow cell!\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtGezQhCBar0"
      },
      "source": [
        "drive.ListFile({'q':\"title='%s.ipynb'\"%asg_name}).GetList()[0]['id']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkqRZdEcBcT4"
      },
      "source": [
        "files.download(submission_file_name)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
